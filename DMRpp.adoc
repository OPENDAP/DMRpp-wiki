= Building and Testing DMR++ Documents
OPeNDAP, Inc.
{docdate}
:appendix-caption: Appenix
:toc: left
:toclevels: 3
:numbered:
:xrefstyle: short
:docinfo: shared
:icons: font
:tabsize: 4
:indent: 4
:source-highlighter: coderay
:coderay-linenums-mode: inline
:prewrap!:
:imagesdir: ./images
:homepage: www.opendap.org
:DMRpp: DMR++
:Miguel Jimenez <mjimenez@opendap.org>:
:James Gallagher <jgallagher@opendap.org>:

// :xrefstyle: short == numbers, full == numbers and titles

////
Pithy version of why we did this. Written by ChatGPT 4o
(https://chatgpt.com/share/680fb929-06a0-8010-a492-47bd11a682bd). jhrg 4/28/25

OPeNDAP developed the DMR++ system to enable fast, efficient access to large scientific datasets without requiring full file downloads. By providing lightweight, annotated metadata, DMR++ reduces data movement and supports scalable, cloud-native workflows essential for modern research.

Some tagline versions:

DMR++: Fast, efficient, cloud-ready access to large scientific datasets.
DMR++: Smarter access to big data — without moving big files.
DMR++: Unlock scientific data faster, with lower cost and complexity.
DMR++: Lightweight metadata for powerful, scalable data access.
////

== Intended Audience
This document is for people who want to enable access to HDF and netCDF data files stored in Amazon Web Services (AWS) Simple Storage Service (S3) using the OPeNDAP _Hyrax_ data server. It describes how to build the {DMRpp} documents the Hyrax server uses. This document will also be useful to people who want to build {DMRpp} documents for other reasons, such as enabling client-side technology like _VirtualiZarr_ to access/subset remote data files  using only HTTP.

[TIP]
There are some features of the {DMRpp} builder software that are targeted specifically to the NASA Earth Science Data and Information Systems (ESDIS) project's use of S3 to store data. Look for the _TIP_ icon to find those.

== Introduction ==

The {DMRpp} is a metadata file that provides a fast and flexible way to access data stored in Amazon Web Services (AWS) Simple Storage Service (S3), or using any other service that supports HTTP.footnote:[The HTTP/S service must support the _Range_ header of HTTP/1.1. When using libcurl, both HTTP/S and the 'file:' protocols can be used.]

// Written by ChatGPT. https://chatgpt.com/share/680fb929-06a0-8010-a492-47bd11a682bd jhrg 4/28/25
OPeNDAP developed the {DMRpp} system in response to the growing need for faster, more efficient access to scientific data stored in large, complex files. As datasets expanded in size and migrated to cloud-based storage systems like S3, traditional methods of reading metadata and retrieving data became increasingly cumbersome and expensive. {DMRpp} was designed to solve this problem by creating a lightweight, annotated metadata file that describes where data elements are located and how they can be accessed, without requiring full downloads or local processing. By decoupling metadata access from the underlying storage and minimizing data movement, {DMRpp} enables scalable, cloud-friendly workflows that better support the demands of modern Earth science research.

The {DMRpp} encodes the location of data residing in a binary data file/object (e.g., an link:https://www.hdfgroup.org/solutions/hdf5/[HDF5] file) so that it can be directly accessed, without the need for an intermediate Application Programmer Interface (API) library. The binary data objects may be in a local filesystem, or they may be accessible using HTTP in, for example, an S3 bucket. The Hyrax data server from OPeNDAP can use {DMRpp} files to provide access and subsetting services for data stored in S3 without first copying their data files from S3. That is, Hyrax can use the {DMRpp} files to access and subset data 'in place.' This mirrors the behavior of the Hyrax server when used with data files stored on POSIX file systems.

The {DMRpp} is an extension of the Dataset Metadata Response (DMR) from OPeNDAP's DAP4 protocol. For a full description of DAP4 and the DMR object, see the DAP4 protocol, link:https://opendap.github.io/dap4-specification/DAP4.html[Sections 1.5.7–1.5.15]. The link:https://opendap.github.io/dap4-specification/DAP4.html#_dmr_declarations[DMR] encodes metadata information about the names, data types, and hierarchical relations of the variables that make up a dataset. The {DMRpp} adds information about the location, size, and other relevant characteristics of those variables. Software can then use this information to read binary data values directly from the dataset's file(s) without using an API library or copying the dataset to temporary storage before accessing the data.

Additional advantages to the {DMRpp} are:

. Enables data providers to take advantage of modern storage technologies for large data without having to reformat huge data collections.

. A {DMRpp} can be programmatically generated by OPeNDAP software for datasets that are made up of HDF5, NetCDF4, HDF4, and HDF4_EOS2 data files.

. Data file integrity is preserved.

[[Diagram]]
.A collection of HDF5 files in an S3 bucket. Each data file has an associated {DMRpp} file, named using the data file name with the suffix '.dmrpp'. Because the {DMRpp} uses a URL to reference the source data file, it can be stored 'close' to the data or on a different storage system.
image::DMRppS3.png[width=70%, align='center']

////
// Revised version. jhrg 4/28/25
== How Does It Work? ==

The {DMRpp} builder software reads a data file and builds a document that captures all the file's metadata, including the names, types, and associated attributes of each variable. This information is stored in a document called the Dataset Metadata Response (DMR). {DMRpp} extends the DMR by adding annotations that specify where each variable's data can be found within the file and how to decode those values. In effect, the {DMRpp} is a specially annotated DMR document.

This additional information enables:

Decoupling the annotated {DMRpp} from the physical location of the granule file.

Storing and transferring {DMRpp} files more efficiently since they are typically much smaller than the data granules they represent.

Reading all of a file’s metadata in a single operation, instead of through the iterative processes required by many APIs.

Referencing source granules via web URLs, making the {DMRpp} file location itself independent and flexible.

Software that understands {DMRpp} content can directly access the data values held in the source granule file. It can do so without retrieving the entire file or processing it locally, even when the granule is stored in a Web Object Store like S3.

If a granule contains multiple variables and only a subset is needed, {DMRpp}-enabled software can retrieve just the bytes associated with the specified variables, further improving efficiency.
////

== Supported Data Formats ==
The software to build {DMRpp} documents currently works with HDF5, netCDF4, HDF4, and HDF4-EOS2 files.footnote:[The netCDF4 format is a subset of HDF5, so HDF5 tools are used for both.] Other formats like Zarr and netCDF3 are not currently supported by the {DMRpp} software, but support could be added if requested.

=== The Gory Details ===
Technologies such as HDF5 are best characterized as tools for defining _self-describing_ data files. These files are widely adopted in scientific domains because they support a diverse range of organizational structures for information. In the case of NASA ESDIS, nearly all the more than 8,600 data collections (encompassing over one billion individual files) define distinct sets of _variables_, effectively making each collection a unique data format. Despite these differences, a small number of API libraries can be used to consistently access the data across all collections.

While we aim to provide support for all possible HDF5, HDF4, etc., data files, there are aspects of the _data models_ these API libraries implement that the current {DMRpp} software does not cover. As of April 2025, support for HDF5, as it is used by the NASA ESDIS collections, is close to complete. The best approach to determining if the OPeNDAP {DMRpp} builder software will work for a given collection is to try it. We suggest picking one or two granules/files and then following the steps outlined here in <<sec-build-them>> followed by the testing process described in <<sec-test-them>>. Are the variables all present? Are the values, or a sampled subset of values, correct?

Support for HDF4 and HDF4-EOS2 data files is much newer, and more work will need to be done on edge cases than for HDF5. However, as of April 2025, the same advice applies to these as to the HDF5 case. Try to build the {DMRpp} and then test the result.

[TIP]
In NASA collections using HDF4-EOS2, geolocation information is often not included within individual data files. This approach minimizes storage requirements by avoiding the repeated storage of redundant information. For instance, a MODIS collection may contain approximately 10,000 files (granules), each referencing geolocation data drawn from a common set of around 120 predefined global regions. To manage this, {DMRpp} generates and stores the geo-referencing information in additional compressed data files, but without an attempt to limit that to the minimum amount of the geo-referencing data. Efforts to optimize the storage of HDF4-EOS2 geo-referencing data are planned and will be prioritized based on user demand.

==== Is My NetCDF File A Version _3_ or Version _4_ File?
OPeNDAP's {DMRpp} software does not currently support netCDF3 files.footnote:[Not supporting netCDF3 is a shame because it's commonly found in older collections of data, and it's one of the simpler data formats.] A complicating factor in building {DMRpp} documents is that it can be hard to tell at a glance if a file is netCDF version 3 or version 4. A file with the suffix _.nc4_ is conventionally recognized as a _netCDF-4_ file. However, the file suffix _.nc_ is ambiguous since it is often used for both _netCDF-3_ and _netCDF-4_ files.

You can use the `ncdump` command to determine if a _netCDF_ file is either classic _netCDF-3_ or _netCDF-4_ http://www.bic.mni.mcgill.ca/users/sean/Docs/netcdf/guide.txn_79.html[(You can learn more in the NetCDF documentation here)]. Here are two files, both using the suffix `.nc` where the first is netCDF3 and the second is netCDF4.

[source,shell,linenums]
----
% ncdump -k fnoc1.nc
classic

% ncdump -k SMAP_L4_SM_aup_20150420T210000_Vv7032_001.nc
netCDF-4
----

[#sec-build-them]
== How to Build {DMRpp} reference files

=== Assumptions
You have:

* Docker installed on your computer and at least a basic understanding of its use.
* Data files in a directory on your computer

[NOTE]
In the following, `%` is the terminal prompt. Only some commands produce output, and for those that do, the output is shown below the command. The paths, etc., on your computer will almost certainly be different.

[#sec-examples]
=== Examples
In this section we jump right into some examples without much explanation. This shows the minimum amount of work needed to build the {DMRpp} and sidecar files. See <<sec-cmd-exp>> for details about the `gen_dmrpp_side_car` command, which is the recommended command for building {DMRpp} documents (April 2025).

Change to the directory that holds your data files and assign an environment variable to the full pathname of that directory. This will streamline some of the later steps in this section. In my case that directory is called `HDF4-dir`, and I used the environment variable 'DATA.'

[#ex-setup]
[source,shell,linenums]
----
% cd HDF4-dir
% export DATA=$(pwd)
% echo $DATA
/Users/jimg/src/opendap/hyrax_git/HDF4-dir
----

Here are the files on my computer in the directory assigned to $DATA

[#ex-dir-listing]
[source,shell]
----
% ls
3B42.19980101.00.7.HDF
3B42.19980101.03.7.HDF
3B42.19980101.06.7.HDF
3B42.19980101.09.7.HDF
3B42.20130111.06.7.HDF
3B42.20130111.09.7.HDF
AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf
AIRS.2009.01.02.L3.RetStd_IR001.v7.0.3.0.G20160024358.hdf
AIRS.2009.01.03.L3.RetStd_IR001.v7.0.3.0.G20160024538.hdf
AMSR_E_L2_Land_V09_200206191023_D.hdf
AMSR_E_L2_Land_V09_200206191112_A.hdf
AMSR_E_L3_SeaIce25km_V15_20020601.hdf
MCD12Q1.A2022001.h10v06.061.2023243073808.hdf
MCD19A1.A2024025.h10v06.061.2024027100206.hdf
MOD10A1F.A2024025.h01v08.061.2024027134335.hdf
MOD10A1F.A2024025.h01v09.061.2024027130238.hdf
MOD10A1F.A2024025.h01v10.061.2024027131939.hdf
MOD11A1.A2024025.h10v06.061.2024028004317.hdf
----

Run the Docker container. The docker run command returns the Container ID (a long hexadecimal string) when the `-d` (run a detached container) is used. The `--name` option sets _hyrax_ as the name of the container which will be used in later commands. Running the container this way enables us to use both build {DMRpp} documents and later test them.

[source,shell,linenum]
----
% docker run -d -h hyrax -p 8080:8080 -v $DATA:/usr/share/hyrax --name=hyrax opendap/hyrax:1.17.1-126
9c88a0d4abe55f17802afd81150280073314f3940b9cd4973ea60dbc43f733a9
----

[NOTE]
In this document, we use an explicit version number when we show the container being used. We do that to make sure that the information here is repeatable. In practice, you can replace that version number with the word _snapshot_ to get the most recent version of the command (and the most recent bundled Hyrax server). That is, where we use `opendap/hyrax:1.17.1-126` using `opendap/hyrax:snapshot` instead will get the most recent version of the software.

[WARNING]
Do not confuse the Docker tag  _snapshot_ with _latest_. In all but the most unusual situations, you do *NOT* want the container tagged _latest_. Use the tag _snapshot_.

If you want to use the latest version of the `gen_dmrpp_side_car` command, replace the version number in _opendap/hyrax:1.17.1-126_ with _snapshot_. Using _opendap/hyrax:snapshot_ will always get the most recent version of the software.

To build a {DMRpp} for the first AIRS file we can run the `gen_dmrpp_side_car` command, using `docker exec`, with the file's name. Because this file is an HDF4 file, the command option `-H` is used.

.Building a {DMRpp} for an AIRS HDF4 file/granule.
[source,shell,linenum,highlight=7]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf -H -U

% ls
...
3B42.20130111.09.7.HDF
AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf
AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf.dmrpp
AIRS.2009.01.02.L3.RetStd_IR001.v7.0.3.0.G20160024358.hdf
...
----

In this second example both the {DMRpp} and a sidecar _missing data_ file (`3B42.19980101.00.7.HDF_mvs.h5`) are built. As is often the case, the {DMRpp} and missing data files together are only 2% of the data file's size.

[NOTE]
Even though the input data file was an HDF4-EOS2 file, the missing data file uses HDF5 to store the values.

This is also an HDF4 file, so the `-H` option is used.

[#ex-missing]
.Building both the {DMRpp} and a missing data file
[source,shell,linenums,highlight=6-7]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i 3B42.19980101.00.7.HDF -H -U

% ls -l
total 1245840
-rw-r--r--@ 1 jimg  staff     774595 Aug 22  2024 3B42.19980101.00.7.HDF
-rw-r--r--  1 jimg  staff       6514 Apr 21 22:42 3B42.19980101.00.7.HDF.dmrpp
-rw-r--r--  1 jimg  staff       8075 Apr 21 22:42 3B42.19980101.00.7.HDF_mvs.h5
-rw-r--r--@ 1 jimg  staff     765742 Aug 22  2024 3B42.19980101.03.7.HDF
 ...
----

The final example in this section shows building a {DMRpp} for an HDF5 file. For an HDF5 file, do not include the `-H` option.

[#ex-hdf5]
.Build a {DMRpp} for an HDF5 file.
[source,shell,linenums,hightlight=7]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5 -U

% ls -l
total 1895576
 ...
-rw-r--r--@ 1 jimg  staff   95114159 Aug  5  2024 SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5
-rw-r--r--  1 jimg  staff     277290 Apr 25 15:51 SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5.dmrpp
----

[#sec-data-source-url]
=== How {DMRpp} References the Matching Data File
[TIP]
This section is primarily for NASA ESDIS users of the {DMRpp} document builder. However, there is some generally useful information here, so most readers should skim it over.

A {DMRpp} document is an eXtensible Markup Language (XML) document. We call the data file/granule that the {DMRpp} describes the _source data file_. Each {DMRpp} has at least one source data file, but may have more than one source data file, for example, with HDF4-EOS2 geo-referencing data.  The first XML _element_ in the {DMRpp} contains a URL that points to the {DMRpp} document's source data file. It looks like this:

[source,xml,linenums,highlight=5]
----
<?xml version="1.0" encoding="ISO-8859-1"?>
<Dataset xmlns="http://xml.opendap.org/ns/DAP/4.0#"
    xmlns:dmrpp="http://xml.opendap.org/dap/dmrpp/1.0.0#" dapVersion="4.0" dmrVersion="1.0"
    name="SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
    dmrpp:href="https://test.opendap.org/examples/SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
    dmrpp:version="3.21.1-243">
----

There are three _XML attributes_ in the root element of the {DMRpp} that are relevant to this discussion. They are:

[source,shell]
----
name="SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
dmrpp:href="https://test.opendap.org/examples/SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
dmrpp:version="3.21.1-243">
----

[horizontal]
name:: The name of the data file/granule.
dmrpp:href:: The full URL to the source data file.
dmrpp:version:: The version of the {DMRpp} builder software used to make this {DMRpp} document.

The value of the `dmrpp:href` attribute is the source of data values that the Hyrax data server will use with building data responses. This URL can be either an HTTP, HTTPS or _file://_ URL (for more about the latter option, see <<sec-test-them>>.

However, when the OPeNDAP {DMRpp} was first developed for use by NASA ESDIS, we did not want to encode the URL to the data file into the {DMRpp}. Instead, we planned on using the ESDIS Common Metadata Repository (CMR) to look up information about a granule and use that to find the source data file. This helped guard against having to edit many of the documents while the ESDIS system was in flux (i.e., it was a decision well aligned with agile development principles). In place of an explicit URL to the source data file, the `gen_dmrpp_side_car` will, by default, use a template string that the hyrax data server substitutes at runtime with the current data source URL as read from CMR.

What if you do not need or want that? The `-u` option of `gen_dmrpp_side_car` provides a way to tell the {DMRpp} document builder to use a specific value for the data source URL. The following examples show the {DMRpp} XML _with_ the template value for the data source URL and then using a URL set with the `-u` option.

.With the template
[source,shell,linenum,highlight=6]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5
%head  SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5.dmrpp
<?xml version="1.0" encoding="ISO-8859-1"?>
<Dataset xmlns="http://xml.opendap.org/ns/DAP/4.0#" xmlns:dmrpp="http://xml.opendap.org/dap/dmrpp/1.0.0#" dapVersion="4.0" dmrVersion="1.0"
    name="SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
    dmrpp:href="OPeNDAP_DMRpp_DATA_ACCESS_URL"
    dmrpp:version="3.21.1-243">
----

The template value for the data source URL is `OPeNDAP_DMRpp_DATA_ACCESS_URL`

.Explicit data source URL, set using `-u`
[source,shell,linenum,highlight=6]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5 -u https://test.opendap.org/examples/SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5
% head SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5.dmrpp
<?xml version="1.0" encoding="ISO-8859-1"?>
<Dataset xmlns="http://xml.opendap.org/ns/DAP/4.0#" xmlns:dmrpp="http://xml.opendap.org/dap/dmrpp/1.0.0#" dapVersion="4.0" dmrVersion="1.0"
    name="SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
    dmrpp:href="https://test.opendap.org/examples/SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
    dmrpp:version="3.21.1-243">
----

The `-u` option provides the literal text for the value of the `dmrpp:href` XML attribute.

[#sec-sidecar-template]
=== How {DMRpp} References a Sidecar File for Geo-referencing Data
The mechanism described above for the data source URL, where the {DMRpp} builder provides a template value unless overridden using the `-u` option, applies similarly to references for sidecar geo-referencing data. By default, the name of the sidecar file itself is used. To include a template value (`OPeNDAP_DMRpp_SC_DATA_ACCESS_URL`) instead, the `-U` option must be specified. As with the data source URL, the `-s` option (described below in <<sec-cmd-exp>>) may be used to explicitly set the sidecar file URL.

There is one exception to the rule that `-u` is used for the data source URL and `-s` is used for the sidecar data file. If `-u` is used, that name will be used as a _pattern_ for the sidecar data file such that the missing data file will be assumed to be named the same as the data source, but with the suffix `_mvs.h5`.

In this example, we show the three files made from an HDF4-EOS2 file that where the sidecar file is necessary. The output of the command is shown first, followed by two views inside the {DMRpp} document.

.An Explicit Data Source URL is a Pattern for an Explicit Sidecar Data URL
[source,shell,linenum,highlight=6-7]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i 3B42.20190110.06.7.HDF -H -u file:///usr/share/hyrax/3B42.20190110.06.7.HDF

% ls -l
total 1895672
-rw-r--r--@ 1 jimg  staff     600255 Aug 22  2024 3B42.20190110.06.7.HDF
-rw-r--r--  1 jimg  staff       6595 Apr 25 17:21 3B42.20190110.06.7.HDF.dmrpp
-rw-r--r--  1 jimg  staff       8075 Apr 25 17:21 3B42.20190110.06.7.HDF_mvs.h5
----

.The Resulting XML, edited. Look for the _file:///_ URLs marked with the comments _HERE_.
[source,xml,linenum,highlight=5,13]
----
<?xml version="1.0" encoding="ISO-8859-1"?>
<Dataset xmlns="http://xml.opendap.org/ns/DAP/4.0#" xmlns:dmrpp="http://xml.opendap.org/dap/dmrpp/1.0.0#"
    dapVersion="4.0" dmrVersion="1.0"
    name="3B42.20190110.06.7.HDF"
    dmrpp:href="file:///usr/share/hyrax/3B42.20190110.06.7.HDF">                    <!-- HERE -->
    <Dimension name="nlon" size="1440"/>
    <Dimension name="nlat" size="400"/>
    <Float32 name="nlat">
        ...
        <dmrpp:chunks compressionType="deflate" deflateLevel="4" fillValue="0" byteOrder="LE">
            <dmrpp:chunkDimensionSizes>400</dmrpp:chunkDimensionSizes>
            <dmrpp:chunk offset="5435" nBytes="636" chunkPositionInArray="[0]"
                href="file:///usr/share/hyrax/3B42.20190110.06.7.HDF_mvs.h5" />     <!-- HERE -->
        </dmrpp:chunks>
        ...
----

[#sec-cmd-exp]
=== Explanation of the `gen_dmrpp_side_car` Command Options
The gen_dmrpp_side_car command takes a few options that control how it builds {DMRpp} and sidecar files.
[horizontal,labelwidth=11]

-i:: The `-i` option is used to name the _input data file_. This data file should be found in the directory where the command is being run, or one of its child directories. In the latter case, the relative pathname to the file should be used. This option is required.

-H:: The `-H` option tells the command that the input file is an HDF4 or HDF4-EOS2 data file. If the `-H` option is not used, then the data file is assumed to be either HDF5 or netCDF4.

-c:: The `-c` option results in {DMRpp} and sidecar files that follow the Climate Forecast (CF) conventions. Using this option provides a {DMRpp} that mimics the behavior of the Hyrax server when it is used to serve data stored on POSIX file systems with the _EnableCF_ option turned on. This organizes the presentation of the variables to follow CF and flattens the internal hierarchy of the data files, hiding any _Groups_.

-D:: The `-D` option will disable the build of a sidecar file, even when one would normally be required. The default is to build sidecar data files when needed.

-U:: Use the template value (`OPeNDAP_DMRpp_SC_DATA_ACCESS_URL`) for the value of the sidecar data file URL. The default is to use only the name of the template file. In most cases, if a sidecar file is made the `-U` or `-u <URL>` options should be used.

-u/--URL:: The `-u/--URL` and `-s/--SURL` options control how URLs are represented in the {DMRpp} document. It is possible to build a {DMRpp} before the location of the data file in S3, for example, is known. In this case, the URL that references the data file will be represented by a 'template' value and substituted into the {DMRpp} _when the document is used_, nominally by the Hyrax service at runtime (although other software can also do this substitution - it is a simple text replacement). See  <<sec-data-source-url>>. If this option is used, no run-time substitution of the data source URL will be performed.

-s/--SURL:: The `-s/--SURL` option provides the same feature for the URL that references the sidecar geo-referencing data file. The Hyrax service _assumes_ that the data file URL can be determined by removing the suffix `.dmrpp` from the {DMRpp} URL. Similarly, it assumes that the sidecar data file URL can be found by replacing the `.dmrpp` suffix with `_mvs.h5`. See <<ex-missing>>. Note that these options can be used to provide real values for data file and sidecar data URls. In that case, the given values will be used in the {DMRpp} instead of the template values. No run-time substitution of the URLs will be performed.

=== Explanation of the `docker run` Command Options
[#sec-docker-exp]
In the  <<sec-examples>> we used one docker command to start a container and then a second docker command to run the {DMRpp} builder inside that container. Here is an explanation of those commands in more detail. First, the container is started on the host computer.

[source,shell,linenum]
----
% docker run -d -h hyrax -p 8080:8080 -v $DATA:/usr/share/hyrax --name=hyrax opendap/hyrax:1.17.1-126
9c88a0d4abe55f17802afd81150280073314f3940b9cd4973ea60dbc43f733a9
----

The `docker run -d ...` command will run the Hyrax container on your computer (called the _host_ computer) in _detached_ mode. The Hyrax container includes both the complete Hyrax service and the `gen_dmrpp_side_car` command. Later this server will be used to test the {DMRpp} documents that are built.

The volume mount, from `$DATA` to `/usr/share/hyrax` mounts the current directory of the host computer running the container to the directory _/usr/share/hyrax_ inside the container. That directory is the root of the Hyrax server's data tree. This means that the data files in the `$DATA` directory will be accessible by the server running in the container without any other configuration.

Complete option summary:
[horizontal]
-d, --detach:: Run container in the background and print container ID
-h, --hostname:: Set the container's host name
-p, --publish:: Publish a container's port(s) to the Docker host
-v, --volume:: Mount a volume so that the container can use files on the Docker host
--name:: Assign a name to the container; this name can be used in later Docker commands

Once running, the container is used to run the command that will build the {DMRpp} document.

[source,shell]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i 3B42.19980101.00.7.HDF -H -U
----

The command that built the {DMRpp} (and sidecar) file really consists of _two commands_. The first is `docker exec -it -w /usr/share/hyrax hyrax` which instructs docker to _execute_ a program in the running container named _hyrax_ and do so by first changing to the directory _/usr/share/hyrax_ in that container. By using the `-w` option we are able to run the gen_dmrpp_side_car command in the directory within the container where data appear.

The second command instructs the docker container to run `gen_dmrpp_side_car` using the arguments `-i 3B42.19980101.00.7.HDF -H -U` which mean use the file _3B42.19980101.00.7.HDF_ as the input data file, assume it is an HDF4 file and use the template name for the sidecar data file.

Complete option summary for the `docker exec` command:
[horizontal]
-i, --interactive:: Set the working directory inside the container
-t, --tty:: Allocate a pseudo-terminal
-w, --workdir:: Set the working directory inside the container

[#sec-test-them]
== Testing {DMRpp} Containers Using the Builtin Hyrax Server
One of the more confounding things about testing {DMRpp} documents is that it requires a data server, or some software component, that can interpret the documents. Instead of the data being directly available, the {DMRpp} sits between the software and the data. In this section we show how to test a {DMRpp} document that using the Hyrax server running in the container used to build the {DMRpp} document. To do this, we will build the {DMRpp} with _file URLs_ for the data and sidecar files instead of _HTTP URLs_ or the _template values_ that the command would normally use.

----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i 3B42.20130111.09.7.HDF -H -u 'file:///usr/share/hyrax/3B42.20130111.09.7.HDF'
----

Copy that pattern for whatever file you use. From the `/usr/share/hyrax` directory, you pass _gen_dmrpp_side_car_ the name of the file (because it's local to the current directory) using the `-i` option. The `-u` option tells the command to embed the URL that follows it in the {DMRpp}. I've used a _file://_  URL to the file _/usr/share/hyrax/3B42.19980101.00.7.HDF_.

NOTE: In the URL above, three slashes follow the colon: two from the way a URL names a protocol and one because the pathname starts at the root directory.

Let's look at how the _hyrax_ service will treat that data file using the {DMRpp}. In a browser, go to  http://localhost:8080/opendap/[http://localhost:8080/opendap/]. The _hyrax_ container must be started using the `docker run` command for this to work (<<sec-examples>>).

.Hyrax Catalog view of all files available.
image::Hyrax-including-new-DMRpp.png[width=650, height=400]

NOTE: The server caches data catalog information for 5 minutes (although this can be configured) so new items (e.g., {DMRpp} documents) may not show up right away. To force the display of a {DMRpp} that you just created, click on the source data file name and edit the URL so that the suffix `.dmr.html` is replaced by `.dmrpp.dmr` .

Click on your equivalent of the `3B42.20130111.09.7.HDF` link, subset, download, and open in Panoply or the equivalent.

.Page view of the DAP _Data Request Form_ for subsetting the dataset.
image::Hyrax-subsetting.png[width=650, height=400]

You can run batch tests in lots of files by building many {DMRpp} documents and then asking the server for various responses (_nc4_, _dap_) from the {DMRpp} and the original file. Those could be compared using various schemes such as the command _getdap4_ included in the container. The `getdap4` command can be used to compare the _dap_ responses from the data file and the {DMRpp} document.

Below is a comparison of the same underlying data, the left window shows the data returned using the {DMRpp}, the right shows the data read directly from the file using the server's builtin HDF4 reader.

.Comparison of responses from a {DMRpp} (left) and the native file handler (right).
image::Data-comparison.png[width=650, height=400]

== Serving Data Using {DMRpp} Files ==
[NOTE]
This is older text that repeats some of the above material, but it provides a good reference for using the {DMRpp} in a range of data provider situations.

There are three fundamental deployment scenarios for using {DMRpp} files to serve data with the Hyrax data server.

This can be simply categorized as follows:
The {DMRpp} file(s) are XML files that contain a root `dap4:Dataset` element with a `dmrpp:href` attribute whose value is one of:

. A http(s):// URL referencing to the underlying granule files via http.

. A file:// URL that references the granule file on the local filesystem in a location that is inside the BES' data root tree.

. The template string `OPeNDAP_DMRpp_DATA_ACCESS_URL`

Each will be discussed in turn below.

NOTE: By default, Hyrax will automatically associate files whose name ends with ".dmrpp" with the *{DMRpp}* handler.

=== Using {DMRpp} with HTTP/S URLs ===

If the {DMRpp} files that you wish to serve contain `dmrpp:href` attributes whose values are http(s) URLs then there are 2+1 steps to serve the data:

. Place the {DMRpp} files on the local disk inside the directory tree identified by the `BES.Catalog.catalog.RootDirectory` in the BES configuration.
. Ensure that the Hyrax `AllowedHosts` list is configured to allow Hyrax to access those target URLs. This can be done by adding new regex records to the `AllowedHosts` list in `/etc/bes/site.conf`, creating that file as need be.
. If the data URLs require authentication to access, then you'll need to configure Hyrax for that too. See link:https://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#_authentication_and_authorization[The Hyrax Data Server Installation and Configuration Guide] for more information.

=== Using {DMRpp} with file URLs ===

Using {DMRpp} files with locally held files can be useful for verifying that {DMRpp} functionality is working without relying on network access that may have data rate limits, authenticated access configuration, or security access constraints. Additionally, in many cases the {DMRpp} access to the locally held data may be faster than through the native `netcdf-4/HDF5` data handlers.

To use {DMRpp} files that contain `file://` URLs:

. Place the {DMRpp} files on the local disk inside the directory tree identified by the `BES.Catalog.catalog.RootDirectory` in the BES configuration.

. Ensure that the {DMRpp} files contain only file:// URLs that refer to data granule files that are inside the directory tree identified by the `BES.Catalog.catalog.RootDirectory` in the BES configuration.

Note: For Hyrax, a correctly formatted file URL must start with the protocol `file://` followed by the full qualified path to the data granule, for example: 

`/usr/share/hyrax/ghrsst/some_granule.h5`

so that the completed URL will have three slashes after the first colon:

`file:///usr/share/hyrax/ghrsst/some_granule.h5`

=== Using {DMRpp} with the template string (NASA). ===
[TIP]
This is most relevant to the operation of the NASA ESDIS Hyrax in the Cloud server deployment.

Another way to serve {DMRpp} files with Hyrax is to build the {DMRpp} files *without* valid URLs but with a template string that is replaced at runtime. If no target URL is supplied to _get_drmpp_ at the time that the {DMRpp} is generated the template string: `*OPeNDAP_DMRpp_DATA_ACCESS_URL*` will be added to the file in place of the URL. The at runtime it can be replaced with the correct value.

Currently, the only implementation of this is Hyrax's NGAP service that, when deployed in the NASA NGAP cloud, will accept _REST__ URLs that are defined as having a URL path component with two mandatory and one optional parameters:

----
 MANDATORY: "/collections/UMM-C:{concept-id}"
 MANDATORY: "/granules/UMM-G:{GranuleUR}"
----

.Example Hyrax in the Cloud REST URL
[source]
----
https://opendap.earthdata.nasa.gov/collections/C1443727145-LAADS/granules/MOD08_D3.A2020308.061.2020309092644.hdf.nc
----

[horizontal]
UMM-C:\{concept-id\}:: /collections/C1443727145-LAADS
UMM-G:\{GranuleUR\}:: /granules/MOD08_D3.A2020308.061.2020309092644.hdf.nc

When encountering this type of URL, Hyrax will decompose it and use the content to formulate a query to the NASA CMR to retrieve the data access URL for the granule and for the {DMRpp} file. It then retrieves the {DMRpp} file and injects the data URL so that data access can proceed as described above.

[TIP]
More on the REST Path can be found https://wiki.earthdata.nasa.gov/display/DUTRAIN/Feature+analysis%3A+Restified+URL+for+OPENDAP+Data+Access[here]. You need access to the NASA ESDIS Earthdata Wiki to follow that link.

[appendix]
== Useful Docker Commands
A useful docker command, `ps`, provides a way to see which docker containers are running.

[source,shell]
----
% docker ps
----
or make a command alais for a more compact listing than the default output of `docker ps`
[source,shell]
----
% alias d-ps='docker ps --format "table {{.ID}}\t{{.Names}}\t{{.Status}}\t{{.Image}}"'
----
This will show a somewhat easier-to-read bit of information about all the running Docker containers on your host:
[source,shell]
----
% d-ps

CCONTAINER ID   NAMES     STATUS          IMAGE
82074fe6ccfe    hyrax     Up 13 minutes   opendap/hyrax:1.17.1-126
----
If you want to stop the container, use
[source,shell]
----
% docker rm -f hyrax
----

[appendix]
== Building {DMRpp} files for HDF4 and HDF4-EOS2 (advanced) ==
[WARNING]
This appendix documents an advanced command (`get_dmrpp_h4`) that is used to build {DMRpp} documents for HDF4 and HDF4-EOS2. For most people we recommend using `gen_dmrpp_side_car` instead. _Caveat emptor_. See <<sec-build-them>> for up-to-date information on building the {DMRpp}.

The HDF4 and HDF4-EOS2 (hereafter just HDF4) {DMRpp} document builder is currently available in the docker container we build for link:https://www.opendap.org/software/hyrax-data-server/[hyrax] server/service. You can get this container from link:https://hub.docker.com/repository/docker/opendap/hyrax[our public Docker Hub repository]. You can also get and build the ''Hyrax'' source code and use the client that way (as part of a source code build), but it's much more complex than getting the Docker container. In addition, the Docker container includes a server that can test the {DMRpp} documents that are built and can even show you how the files would look when served without using the {DMRpp}.

NOTE: The following commands should be considered still experimental and subject to some change. Modify it to suit your own needs.

=== Using get_dmrpp_h4 ===
Make a new directory in a convenient place and copy the HDF4 and/or HDF4-EOS2 files in that directory. Once you have the files in that directory, make an environment variable so it can be referred to easily. From inside the directory:

----
export HDF4_DIR=$(pwd)
----

Get the Docker container from Docker Hub using this command:

----
docker run -d -h hyrax -p 8080:8080 -v $HDF4_DIR:/usr/share/hyrax --name=hyrax opendap/hyrax:snapshot
----

What the options mean:

----
-d, --detach Run container in background and print container ID
-h, --hostname Container host name
-p, --publish Publish a container's port(s) to the host
-v, --volume Bind mount a volume
--name Assign a name to the container
----

This command will fetch the container *opendap/hyrax:snapshot* from Docker Hub. The _snapshot_ is the most recent build of the container. It will then _run_ the container and return the container ID. The _hyrax_ server is now running on your computer and can be accessed with a web browser, curl, etc. More on that in a bit.

The volume mount, from `$HDF4_DIR` to `'/usr/share/hyrax'` mounts the current directory of the host computer running the container to the directory _/usr/share/hyrax_ inside the container. That directory is the root of the server's data tree. This means that the HDF4 files you copied into the `HDF4_DIR` directory will be accessible by the server running in the container. That will be useful for testing later on.

Note: If you want to use a specific container version, substitute the version info for _snapshot._

==== Running the {DMRpp} builder ====

NOTE: At the end of this, I'll include a shell script that takes away many of these steps. However, the script obscures some aspects of the command that you might want to tweak, so the following shows you all the details. Skip to *Simple shell command* to skip over these details.

Make sure you are in the directory with the HDF4 files for these steps.

Get the command to return its help information:

----
docker exec -it hyrax get_dmrpp_h4 -h
----

will return:

----
usage: get_dmrpp_h4 [-h] -i I [-c CONF] [-s] [-u DATA_URL] [-D] [-v]

Build a dmrpp file for an HDF4 file. get_dmrpp_h4 -i h4_file_name. A dmrpp
file that uses the HDF4 file name will be generated.

optional arguments:

...
----

Let's build a {DMRpp} now, by explicitly using the container:

----
docker exec -it hyrax bash
----

starts the _bash_ shell in the container, with the current directory as root (/)

----
[root@hyrax /]#
----

Change to the directory that is the root of the data (you'll see your HDF4 files in here):

----
 cd /usr/share/hyrax
----

You will see, roughly:

----
[root@hyrax /]# cd /usr/share/hyrax
[root@hyrax hyrax]# ls
3B42.19980101.00.7.HDF
3B42.19980101.03.7.HDF
3B42.19980101.06.7.HDF
...
----

In that directory, use the _get_dmrpp_h4_ command to build a {DMRpp} document for one of the files:

----
[root@hyrax hyrax]# get_dmrpp_h4 -i 3B42.20130111.09.7.HDF -u 'file:///usr/share/hyrax/3B42.20130111.09.7.HDF'
----

Copy that pattern for whatever file you use. From the `/usr/share/hyrax` directory, you pass _get_dmrpp_h4_ the name of the file (because it's local to the current directory) using the *-i* option. The *-u* option tells the command to embed the URL that follows it in the {DMRpp}. I've used a _file://_  URL to the file _/usr/share/hyrax/3B42.19980101.00.7.HDF_.

NOTE: In the URL above, three slashes follow the colon: two from the way a URL names a protocol and one because the pathname starts at the root directory.

Building the {DMRpp} and embedding a _file://_ URL will enable testing the {DMRpp}.

==== Simple shell command ====

Here is a simple shell command that you can run on the host computer that will eliminate most of the above.

NOTE: ''In the spirit of a recipe, I'll restate the earlier command for starting the docker container with the *get_dmrpp_h4* command and the *hyrax* server.''

Start the container:

----
docker run -d -h hyrax -p 8080:8080 -v $HDF4_DIR:/usr/share/hyrax --name=hyrax opendap/hyrax:snapshot
----

Check if it is running:

----
docker ps
----

The command, written for the Bourne Shell, is:

----
#!/bin/sh
#
# usage get_dmrpp_h4.sh <file>

data_root=/usr/share/hyrax

cat <<EOF | docker exec --interactive hyrax sh
cd $data_root
get_dmrpp_h4 -i $1 -u "file://$data_root/$1"
EOF
----

Copy that, save it in a file (I named the file _get_dmrpp_h4.sh_).

Run the command on the host, not the docker container, and in the directory with the HDF4 files (you don't have to do that, but sorting out the details is left as an exercise for the reader. Run the command like this:

----
 ./get_dmrpp_h4.sh AMSR_E_L3_SeaIce25km_V15_20020601.hdf
----

The {DMRpp} will appear when the command completes.

----
(hyrax500) hyrax_git/HDF4-dir % ls -l
total 1251240
-rw-r--r--@ 1 jimg  staff    1250778 Aug 22 22:31 AMSR_E_L2_Land_V09_200206191112_A.hdf
-rw-r--r--@ 1 jimg  staff   20746207 Aug 22 22:32 AMSR_E_L3_SeaIce25km_V15_20020601.hdf
-rw-r--r--  1 jimg  staff    3378674 Aug 28 17:37 AMSR_E_L3_SeaIce25km_V15_20020601.hdf.dmrpp
----

[appendix]
== Building {DMRpp} files for HDF5/NetCDF4 with _get_dmrpp_ ==
[WARNING]
This appendix documents a deprecated command (`get_dmrpp`) that was used to build {DMRpp} documents for HDF5 and netCDF4 before `gen_dmrpp_side_car` was introduced. _Caveat emptor_.  See <<sec-build-them>> for up-to-date information on building the {DMRpp}.

The application that builds the {DMRpp} files is a command line tool called _get_dmrpp_. It in turn uses other executables such as _build_dmrpp_, _reduce_mdf_, _merge_dmrpp_ (which rely in turn on the _HDF5_handler_ and the HDF5 library), along with a number of UNIX shell commands.

All of these components are installed with each recent version of the Hyrax Data Server

You can see the _get_dmrpp_ usage statement with the command:

----
get_dmrpp -h
----

=== Using _get_dmrpp_ ===

The way that _get_dmrpp_ is invoked controls the way that the data are ultimately represented in the resulting {DMRpp} file(s).

The _get_dmrpp_ application uses software from the Hyrax data server to produce the base DMR document which is used to construct the {DMRpp} file.

The Hyrax server has a long list of configuration options, several of which can substantially alter the structural and semantic representation of the dataset as seen in the {DMRpp} files generated using these options.

=== Command line options ===

The command line switches provide a way to control the output of the tool. In addition to common options like verbose output or testing modes, the tool provides options to build extra (aka 'sidecar') data files that hold information needed for CF compliance. See the ''missing data'' section for more information. In addition, it is often desirable to build {DMRpp} files before the source data files are uploaded to a cloud store like S3. In this case, the URL to the data may not be known when the {DMRpp} is built. We support this by using placeholder/template strings in the ''dmr++'' and which can then be replaced with the URL at runtime, when the {DMRpp} file is evaluated. See the '-u' and '-p' options below.

==== Inputs ====

*-b* ::
The fully qualified path to the top-level data directory. Data files read by _get_dmrpp_ must be in the directory tree rooted at this location and their names expressed as a path relative to this location. The value may not be set to `/` , or `/etc`. The default value is `/tmp` if a value is not provided. All the data files to be processed must be in this directory or one of its subdirectories. If _get_dmrpp_ is being executed from same directory as the data then `-b `pwd`` or `-b .` works as well.

*-u* ::
This option is used to specify the location of the binary data object. Its value must be `http`, `https`, or a `file://` URL. This URL will be injected into the {DMRpp} when it is constructed. If option `-u` is not used; then the template string `OPeNDAP_DMRpp_DATA_ACCESS_URL` will be used and the {DMRpp} will substitute a value at runtime.

*-c* ::
The path to an alternate bes configuration file to use.

*-s* ::
The path to an optional addendum configuration file which will be appended to the default BES configuration. Much like the `site.conf` file works for the full server deployment it will be loaded last and the settings there-in will affect the default configuration.


==== Output ====

*-o* ::
The name of the file to create.

==== Verbose Output Modes ====

*-h* ::
Show the help/usage page.
*-v* ::
verbose mode, prints the intermediate DMR.
*-V* ::
Very verbose mode, prints the DMR, the command, and the configuration file used to build the DMR.
*-D* ::
Print the DMR that will be used to build the {DMRpp}.
*-X* ::
Do not remove temporary files. May be used independently of the `-v` and/or `-V` options.


==== Tests ====

*-T* ::
Run ALL hyrax tests on the resulting {DMRpp} file and compare the responses the ones generated by the source HDF5 file.
*-I* ::
Run hyrax inventory tests on the resulting {DMRpp} file and compare the responses the ones generated by the source HDF5 file.
*-F* ::
Run hyrax value probe tests on the resulting {DMRpp} file and compare the responses the ones generated by the source HDF5 file.

==== Missing Data Creation ====

*-M* ::
Build a 'sidecar' file that holds missing information needed for CF compliance (e.g., Latitude, Longitude and Time coordinate data).
*-p* ::
Provide the URL for the Missing data sidecar file. If this is not given (but -M is), then a template value is used in the {DMRpp} file and a real URL is substituted at runtime.
*-r* ::
The path to the file that contains missing variable information for sets of input data files that share common missing variables. The file will be created if it doesn't exist and the result may be used in later invocations of _get_dmrpp_ (using `-r`) to identify the missing variable file.

==== AWS Integration ====
The _get_dmrpp_ application supports both S3 hosted granules as inputs, and uploading generated {DMRpp} files to an S3 bucket.

*S3 Hosted granules are supported by default* ::
When the `get_dmrpp` application sees that the name of the input file is an S3 URL it will check to see if the AWS CLI is configured and if so `get_dmrpp` will attempt retrieve the granule and make a {DMRpp} utilizing whatever other options have been chosen. **For example:**

	get_dmrpp -b `pwd` s3://bucket_name/granule_object_id


*-U* ::
The `-U` command line parameter for `get_dmrpp` instructs `get_dmrpp` application to upload the generated {DMRpp} file to S3, but only when the following conditions are met:
- The name of the input file is an S3 URL.
- The `AWS CLI` has been configured with credentials that provide `r+w` permissions for the bucket referenced in the input file S3 URL.
- The `-U` option has been specified.
If all three of the above are true then `get_dmrpp` will copy the retrieve the granule, create a {DMRpp} file from the granule, and copy the resulting {DMRpp} file (as defined by the `-o` option) to the source S3 bucket using the well-known NGAP sidecar file naming convention: *s3://bucket_name/granule_object_id.dmrpp*.  For example:

	get_dmrpp -U -o foo -b `pwd` s3://bucket_name/granule_object_id

=== _HDF5_handler_ Configuration ===

Because _get_dmrpp_ uses the _HDF5_handler_ software to build the {DMRpp} the software must inject the _HDF5_handler_'s configuration.

The default configuration is large, but any valued may be altered at runtime.

Here are some of the commonly manipulated configuration parameters with their default values:

----
 H5.EnableCF=false
 H5.EnableDMR64bitInt=true
 H5.DefaultHandleDimension=true
 H5.KeepVarLeadingUnderscore=false
 H5.EnableCheckNameClashing=true
 H5.EnableAddPathAttrs=true
 H5.EnableDropLongString=true
 H5.DisableStructMetaAttr=true
 H5.EnableFillValueCheck=true
 H5.CheckIgnoreObj=false
----

// NOTE: Mikejmnez. It states here that H5.EnableCF is `true` by default. But
// below it states that it is `false` by default...
// I changed this to say 'false' above because I think that's the case for code
// people can get now. jhrg 4/28/25

==== Note to DAACs with existing Hyrax deployments. ====

If your group is already serving data with Hyrax and the data representations that are generated by your Hyrax server are satisfactory, then a careful inspection of the localized configuration, typically held in `/etc/bes/site.conf`, will help you determine what configuration state you may need to inject into _get_dmrpp_.

=== The _H5.EnableCF_ option ===

Of particular importance is the _H5.EnableCF_ option, which instructs the _get_dmrpp_ tool to produce https://cfconventions.org/[Climate Forecast convention (CF)] compatible output based on metadata found in the granule file being processed.

Changing the value of _H5.EnableCF_ from *false* to *true* will have (at least) two significant effects.

It will:

- Cause _get_dmrpp_ to attempt to make the dmr++ metadata CF compliant.
- Remove Group hierarchies (if any) in the underlying data granule by flattening the Group hierarchy into the variable names.

By default _get_dmrpp_ the _H5.EnableCF_ option is set to false:

----
 H5.EnableCF = false
----

There is a much more comprehensive discussion of this key feature, and others, in the https://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#HDF5-handler[HDF5 Handler section] of the Appendix in the Hyrax Data Server Installation and Configuration Guide.

=== Missing data, the CF conventions and _HDF5_ ===

Many of the _HDF5_ files produced by NASA and others do not contain the domain coordinate data (such as latitude, longitude, time, etc.) as a collection of explicit values. Instead, information contained in the dataset metadata can be used to reproduce these values.

In order for a dataset to be Climate Forecast (CF) compatible, it must contain these domain coordinate data values.

The Hyrax _HDF5_handler_ software, used by the _get_dmrpp_ application, can create this data from the dataset metadata.  The _get_dmrpp_ application places these generated data in a “sidecar” file for deployment with the source _HDF5/netcdf-4_ file.


////
[appendix]
=== HDF5 ===
// TODO Rewrite this since, at this point, all known NASA HDF5 files are supported. 4/22/25
The HDF5 data format is quite complex, and many of the options and edge cases are not currently supported by the {DMRpp} software.

These limitations and how to quickly evaluate a HDF5 or netCDF4 file for use with the {DMRpp} software are explained below.

==== HDF5 filters ====

The HDF5 format has several filter/compression options used for storing data values.
The {DMRpp} software currently supports data that use the H5Z_FILTER_DEFLATE, H5Z_FILTER_SHUFFLE, and H5Z_FILTER_FLETCHER32 filters.
https://support.hdfgroup.org/documentation/HDF5/latest/group___h5_z.html[You can find more on HDF5 filters here].

==== HDF5 storage layouts ====

The HDF5 format also uses a number of "storage layouts" that describe various structural organizations of the data values associated with a variable in the granule file.
The {DMRpp} software currently supports data that use the H5D_COMPACT, H5D_CHUNKED, and H5D_CONTIGUOUS storage layouts. These are all the storage layouts defined by the HDF5 library, but others can be added.
https://support.hdfgroup.org/releases/HDF5/v1_16/v1_16_0/documentation/doxygen/_l_b_dset_layout.html[You can find more on HDF5 storage layouts here].

==== Is my HDF5 or netCDF4 file suitable for {DMRpp}?
To get a human-readable assessment of the file that will show the storage layouts, chunking structure, and the filters needed for each variable (aka DATASET in the _HDF5_ vocabulary), use the https://support.hdfgroup.org/ftp/HDF5/documentation/doc1.6/Tools.html#Tools-Dump[h5dump] command line program.

.h5dump example output
[source,shell]
----
$ h5dump -H -p chunked_gzipped_fourD.h5
HDF5 "chunked_gzipped_fourD.h5" {
	GROUP "/" {
		DATASET "d_16_gzipped_chunks" {
			DATATYPE  H5T_IEEE_F32LE
			DATASPACE  SIMPLE { ( 40, 40, 40, 40 ) / ( 40, 40, 40, 40 ) }
			STORAGE_LAYOUT {
				CHUNKED ( 20, 20, 20, 20 )
				SIZE 2863311 (3.576:1 COMPRESSION)
			}
			FILTERS {
				COMPRESSION DEFLATE { LEVEL 6 }
			}
			FILLVALUE {
				FILL_TIME H5D_FILL_TIME_ALLOC
				VALUE  H5D_FILL_VALUE_DEFAULT
			}
			ALLOCATION_TIME {
				H5D_ALLOC_TIME_INCR
			}
		}
	}
}
----

=== HDF4 and HDF4-EOS2 ===
The internal data storage layout in an HDF4 file is more complex than that in an HDF5 file, and we're focusing on complete support for those features used by NASA. In addition, we also support HDF4-EOS2, data files that should be read with the HDF4-EOS2 library. The main reason for using the HDF-EOS2 API is to retrieve the values for the Domain variables such as Latitude and Longitude. Our support handles the HDF4-EOS Grid data type and uses {DMRpp} to retrieve the Latitude and Longitude values appear as users expect. Dmrpp can handle hDF-EOS2 swath. However, for some HDF-EOS2 MODIS swath (level 1B etc.), currently users need to find the corresponding HDF-EOS2 files (MODIS level 3 products) that store the actual latitude and longitude values for each data point. The {Dmrpp} module doesn't support automatic merging of the latitude and longitude of such HDF-EOS2 swath data.
////

// TODO Should we retain this and update it? jhrg 4/28/25
////
== Recipe: Building and testing {DMRpp} files ==
There are two recipes shown here, the first using a Hyrax docker container, and a second using the container that is part of the NASA EOSDIS Cumulus task.

*_Prerequisites_*:

- The Docker daemon running on a system that also supports a shell (the examples use bash in this section).

=== Recipe: Building {DMRpp} files using a Hyrax docker container ===

. Acquire representative granule files for the collection you wish to import. Put them on the system running the Docker daemon. For this recipe we will assume that these files have been placed in the directory:

	/tmp/dmrpp

. Get the most up-to-date Hyrax docker image:

	docker pull opendap/hyrax:snapshot

. Start the docker container, mounting your data directory on to the docker image at `/usr/share/hyrax`:

	docker run -d -h hyrax -p 8080:8080 --volume /tmp/dmrpp:/usr/share/hyrax --name=hyrax opendap/hyrax:snapshot

. Get a first view of your data using `get_dmrpp` with its default configuration.

.. If you want you can build a {DMRpp} for an example "input_file" using a docker exec command:

	docker exec -it hyrax get_dmrpp -b /usr/share/hyrax -o /usr/share/hyrax/input_file.dmrpp -u "file:///usr/share/hyrax/input_file" "input_file"

.. Or if you want more scripting flexibility, you can log in to the docker container to do the same:

... Login to the docker container:

	docker exec -it hyrax /bin/bash

... Change working dir to data dir:

	cd /usr/share/hyrax

... Set the data directory to the current one (`-b $(pwd)`) and set the data URL (`-u`) to the fully qualified path to the input file.

	get_dmrpp -b $(pwd) -o foo.dmrpp -u "file://"$(pwd)"/your_test_file" "your_test_file"

NOTE: Now that you have made a dmr++ file, use the running Hyrax server to view and test it by pointing your browser at: http://localhost:8080/opendap/

[start=5]
. You can also batch process all of your test granules if you want to go that route. The following script assumes your source data files end with '.h5'.

NOTE: The resulting *{DMRpp}* files should contain the correct file:// URLs and be correctly located so that they may be tested with the Hyrax service running in the docker instance.

----
#!/bin/bash
# This script will write each output file as a sidecar file into
# the same directory as its associated input granule data file.

# The target directory to search for data files
target_dir=/usr/share/hyrax
echo "target_dir: $target_dir";

# Search the target_dir for names matching the regex \*.h5
for infile in `find "$target_dir" -name \*.h5`
do
    echo " Processing: $infile"

    infile_base=`basename "${infile}"`
    echo "infile_base: $infile_base"

    bes_dir=`dirname "${infile}"`
    echo "    bes_dir: $bes_dir"

    outfile="$infile.dmrpp"
    echo "     Output: $outfile"

    get_dmrpp -b "$bes_dir" -o "$outfile" -u "file://$infile" "$infile_base"
done
----

TIP: Remember that you can use the Hyrax server that is running in the docker container to view and test the {DMRpp} files you just created by pointing your browser at: http://localhost:8080/opendap/


=== Testing and qualifying {DMRpp} files ===
In the previous section/step we created some initial {DMRpp} files using the default configuration. It is crucial to make sure that they provide the representation of the data that you and your users are expecting and that they will work correctly with the Hyrax server. (See the following sections for details). If the generated {DMRpp} files do not match expectations, then the default configuration of the `get_dmrpp` may need to be amended using the `-s` parameter.
If the data are currently being served by your DAAC's on-prem team, this is where understanding exactly what the localizations made to the configurations of the on-prem Hyrax instances deployed for the collection is important. This localization will probably need to be injected into `get_drmpp` to produce the correct data representation in the {DMRpp} files.


=== Flattening Groups ===
By default, `get_dmrpp` will preserve and show group hierarchies. If this is not desired, say for CF-1.0 compatibility, then you can change this by creating a small amendment to `get_dmrpp`'s default configuration.

First, create the amending configuration file:

	echo "H5.EnableCF=true" > site.conf

Then, change the invocation of `get_dmrpp` in the above example by adding the `-s` switch:

	get_dmrpp -s site.conf -b `pwd` -o "$dmrpp_file" -u "file://"`pwd`"/$file" "$file"

And re-run the {DMRpp} production as shown above.



=== DAP representations ===
We have test and assurance procedures for the DAP4 and DAP2 protocols below. Both are important. For legacy datasets the DAP2 request API is widely used by an existing client base and should continue to be supported. Since DAP4 subsumes DAP2 (but with somewhat different API semantics), it should be checked for legacy datasets as well. For more modern datasets that content DAP4 types such as Int64 that are not part of the DAP2 specification or implementations, we will need to rely on eliding the instances of unmapped types or return an error when this is encountered.


----
# Test Constants:
GRANULE_FILE="some_name.h5"
# Granule URL
gf_url="http://localhost:8080/opendap/$GRANULE_FILE"
----



==== Inspect the {DMRpp} files ====

Do the {DMRpp} files have the expected `dmrpp:href` URL(s)?

	head -2 "$GRANULE_FILE.dmrpp"

==== Check DAP4 DMR Response ====
Inspect `$gf_url.dmrpp.dmr`

. Get the document, save as `foo.dmr`:

	curl -L -o foo.dmr "$gf_url.dmr"

. Is each variable's data type correct and as expected?
. Are the associated dimensions correct?


==== DAP4 Check binary data response ====

For a particular granule named GRANULE_FILE and a particular variable, named VARIABLE_NAME (Where VARIABLE_NAME is a https://opendap.github.io/dap4-specification/DAP4.html#_fully_qualified_names[full qualified DAP4 name]):

    curl -L -o dap4_subset_file "$gf_url.dap?dap4.ce=VARIABLE_NAME"
    curl -L -o dap4_subset_dmrpp "$gf_url.dmrpp.dap?dap4.ce=VARIABLE_NAME"
    cmp dap4_subset_file dap4_subset_dmrpp


==== DAP4 UI test ====

View and exercise the DAP4 Data Request Form `$gf_url.dmr.html` with a browser.

==== DAP2 Check DDS Response ====

. Inspect `$gf_url.dds`
.. Is each variable's data type correct and as expected?
.. Are the associated dimensions correct?
. Compare {DMRpp} DDS with granule file DDS -
For a particular granule named GRANULE_FILE and a particular variable named VARIABLE_NAME (Where VARIABLE_NAME is a https://zenodo.org/records/10794666[DAP2 name]):

    curl -L -o dap2_dds_file "$gf_url.dds"
    curl -L -o dap2_dds_dmrpp "$gf_url.dds"
    cmp dap2_dds_file dap2_dds_dmrpp


==== DAP2 Check binary data response ====

For a particular granule named GRANULE_FILE and a particular variable, VARIABLE_NAME (Where VARIABLE_NAME is a https://zenodo.org/records/10794666[DAP2 name]):


    curl -L -o dap2_subset_file "$gf_url.dods?VARIABLE_NAME"
    curl -L -o dap2_subset_dmrpp "$gf_url.dmrpp.dods?VARIABLE_NAME"
    cmp dap2_subset_file dap2_subset_dmrpp

NOTE: One might consider doing this with two or more variables.

==== DAP2 UI Test ====

. View and exercise the DAP2 Data Request Form located here: `$gf_url.html`.
. Try it in Panoply!
.. Open Panoply.
.. From the File menu select *Open Remote Dataset*...
.. Paste the `$gf_url.html` into the resulting dialog box.
////
