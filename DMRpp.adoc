= image:logo-hyrax-red.svg[width=300]
OPeNDAP, Inc.
{docdatetime}
:toc:
:toclevels: 3
:numbered:
:docinfo: shared
:icons: font
:tabsize: 4
:indent: 4
:doctype: book
:source-highlighter: coderay
:coderay-linenums-mode: inline
:prewrap!:
:imagesdir: ./images
:homepage: www.opendap.org
:DMRpp: DMR++
:Miguel Jimenez <mjimenez@opendap.org>:
:James Gallagher <jgallagher@opendap.org>:

= DMR++

== Introduction ==

The {DMRpp} is a metadata file that provides a fast and flexible way to serve data stored in S3, or in any other service that supports HTTP Range GET.

The {DMRpp} encodes the location of the data content residing in a binary data file/object (e.g., a https://support.hdfgroup.org/documentation/HDF5/latest/[HDF5] file) so that it can be directly accessed, without the need for an intermediate library API, by using the file with the location information. The binary data objects may be on a local filesystem, or they may reside across the web in something like an S3 bucket.

A DMR is a metadata description of a datafile and is defined in link:https://opendap.github.io/dap4-specification/DAP4.html[the DAP4 protocol, Sections 1.5.7–1.5.15].
The {DMRpp} adds to the _DMR_ of the datafile _extra_ information (thus the+++ ++ +++), such as `byte offsets`, `chunk references`, `compression`, among other relevant information that can be used to get cloud-performant access to datafiles on S3. Hyrax, the OPeNDAP data Server, uses {DMRpp} to efficiently subset data on a local filesystem, in S3, or any other service that supports HTTP 1.1 or newer.

Additional advantages to the {DMRpp} are:

. Relieves the burden from Data Producers to _transform and reformat_ massive dataset collections, avoiding the need to store multiple copies of the same files.

. All chunks inside the `h5` file remain inside the file, i.e., each file retains its integrity. The location of the chunks inside the `h5` file are specified within the {DMRpp}.

. A {DMRpp} can be easily and programmatically generated by OPeNDAP's Hyrax Data Server for HDF5/NetCDF4 files and most of HDF4 files.

[[note1]]
NOTE: The OPeNDAP software currently supports HDF5, NetCDF4, HDF4, and HDF4-EOS2. Other formats can be supported, such as Zarr.

[[Diagram]]
.A collection of HDF5 files in an S3 bucket. Each file has next to it a sidecar file with the same name as the original but with the suffix ".dmrpp" (i.e., the {DMRpp} file). The {DMRpp} file need not be next to the source file. In fact, because it is typically much smaller than the source file, it is often moved in and out of S3 by the Hyrax data server to reduce latency within S3.
image::DMRppS3.png[width=70%, align='center']

== How Does It Work? ==
The {DMRpp} ingest software reads a data file and builds a document that holds all the file's metadata, the names and types of all the variables along with any other information bound to those variables. This information is stored in a document we call the Dataset Metadata Response (DMR). The {DMRpp} adds some extra information to this regarding where each variable can be found and how to decode those values. The {DMRpp} is simply a special annotated DMR document.

This additional information enables:

* Decoupling the annotated {DMRpp} from the location of the granule file itself.
* Since {DMRpp} files are typically significantly smaller than the source data granules they represent, they can be stored and moved for less expense.
* Reading all the file's metadata in one operation instead of the iterative process that many APIs require.
* If the {DMRpp} contains references to the source granules location on the web, the location of the {DMRpp} file itself does not matter.

Software that understands the {DMRpp} content can directly access the data values held in the source granule file. It can do so without having to retrieve the entire file and work on it locally, even when the file is stored in a Web Object Store like S3.

If the granule file contains multiple variables and only a subset of them are needed, the {DMRpp} enabled software can retrieve just the bytes associated with the specified subset(s) of desired variable(s).

== Supported Data Formats ==

The {DMRpp} software currently works with HDF5, netCDF4, and HDF4/HDF4-EOS2 files. The netCDF4 format is a subset of HDF5, so HDF5 tools are used for both. Other formats like Zarr and netCDF3 are not currently supported by the {DMRpp} software, but support could be added if requested.

However, while Zarr is not currently supported by Hyrax as a data source, the Zarr API can be used to read from data described by the {DMRpp}. An external group working on the Python Kerchunk software has developed https://virtualizarr.readthedocs.io/en/latest/[VirtualiZarr] which can parse either Kerchunk or {DMRpp} documents and read from data those describe using the Zarr API.

=== HDF5 ===
// No content editing yet from here onward to ***jhrg*** below. jhrg 4/19/25
The HDF5 data format is quite complex, and many of the options and edge cases are not currently supported by the {DMRpp} software.

These limitations and how to quickly evaluate a HDF5 or netCDF4 file for use with the {DMRpp} software are explained below.

==== HDF5 filters ====

The HDF5 format has several filter/compression options used for storing data values. 
The {DMRpp} software currently supports data that utilize the  H5Z_FILTER_DEFLATE, H5Z_FILTER_SHUFFLE, and H5Z_FILTER_FLETCHER32 filters.
https://support.hdfgroup.org/documentation/HDF5/latest/group___h5_z.html[You can find more on HDF5 filters here.]

==== HDF5 storage layouts ====

The HDF5 format also uses a number of "storage layouts" that describe various structural organizations of the data values associated with a variable in the granule file.
The {DMRpp} software currently supports data that utilize the  H5D_COMPACT, H5D_CHUNKED, and H5D_CONTIGUOUS storage layouts. These are all the storage layouts defined by the HDF5 library, but others can be added.
https://support.hdfgroup.org/releases/HDF5/v1_16/v1_16_0/documentation/doxygen/_l_b_dset_layout.html[You can find more on HDF5 storage layouts here.]


==== Is my HDF5 or netCDF4 file suitable for {DMRpp}? ====

To determine the HDF5 filters, storage layouts, and chunking scheme used in a HDF5 or netCDF4 file, you can use the command:

------------------------
h5dump -H -p <filename>
------------------------

To get a human-readable assessment of the file that will show the storage layouts, chunking structure, and the filters needed for each variable (aka DATASET in the _HDF5_ vocabulary), use the https://support.hdfgroup.org/ftp/HDF5/documentation/doc1.6/Tools.html#Tools-Dump[h5dump] command line program.

.h5dump example output
[source,sh]
----
$ h5dump -H -p chunked_gzipped_fourD.h5
HDF5 "chunked_gzipped_fourD.h5" {
	GROUP "/" {
		DATASET "d_16_gzipped_chunks" {
			DATATYPE  H5T_IEEE_F32LE
			DATASPACE  SIMPLE { ( 40, 40, 40, 40 ) / ( 40, 40, 40, 40 ) }
			STORAGE_LAYOUT {
				CHUNKED ( 20, 20, 20, 20 )
				SIZE 2863311 (3.576:1 COMPRESSION)
			}
			FILTERS {
				COMPRESSION DEFLATE { LEVEL 6 }
			}
			FILLVALUE {
				FILL_TIME H5D_FILL_TIME_ALLOC
				VALUE  H5D_FILL_VALUE_DEFAULT
			}
			ALLOCATION_TIME {
				H5D_ALLOC_TIME_INCR
			}
		}
	}
}
----

==== Is my netcdf file _netcdf-3_ or _netcdf-4_? ====

A file with the suffix _.nc4_ is recognized as a _netcdf-4_ file. However, the file suffix _.nc_ can be the commonly used naming convention for both _netcdf-3_ and _netcdf-4_ files. You can use the command:  

--------------------
ncdump -k <filename>
--------------------

to determine if a _netcdf_ file is either classic _netcdf-3_ (classic) or _netcdf-4_. http://www.bic.mni.mcgill.ca/users/sean/Docs/netcdf/guide.txn_79.html[You can learn more in the NetCDF documentation here.]

NOTE: The _netcdf_ library must be installed on the system upon which the command is issued.

=== _HDF4_/_HDF4-EOS2_ ===

This is a complicated case, and its support as of 8/29/24 is still considered experimental. The HDF4 data model is quite complex, more so than the HDF5 model, and we're focusing on complete support for those features used by NASA. To this end, we are also working on support for HDF4-EOS2, data files that can only be read correctly with the HDF4-EOS2 library. The main distinction of that API is the treatment of values for the Domain variables for Latitude and Longitude. Our support handles the HDF4-EOS Grid data type and using {DMRpp} the Latitude and Longitude values appear as users expect, although some aspects of this are ongoing. We do not yet support the HDF4-EOS2 Swath data type.

Se the section below for information on the tool for building {DMRpp} files for HDF4 and HDF4-EOS2 data files.

== Building {DMRpp} files for HDF4 and HDF4-EOS2 (experimental) ==
The HDF4 and HDF4-EOS2 (hereafter just HDF4) {DMRpp} document builder is currently available in the docker container we build for link:https://www.opendap.org/software/hyrax-data-server/[hyrax] server/service. You can get this container from link:https://hub.docker.com/repository/docker/opendap/hyrax[our public Docker Hub repository]. You can also get and build the ''Hyrax'' source code, and use the client that way (as part of a source code build), but it's much more complex than getting the Docker container. In addition, the Docker container includes a server that can test the {DMRpp} documents that are built and can even show you how the files would look when served without using the {DMRpp}.


NOTE: The following commands should be considered still experimental and subject to some change. Modify it to suit your own needs.


=== Using get_dmrpp_h4 ===
Make a new directory in a convenient place and copy the HDF4 and/or HDF4-EOS2 files in that directory. Once you have the files in that directory, make an environment variable so it can be referred to easily. From inside the directory:

----------------------
export HDF4_DIR=$(pwd)
----------------------

Get the Docker container from Docker Hub using this command:

------------------------------------------------------------------------------------------------------
docker run -d -h hyrax -p 8080:8080 -v $HDF4_DIR:/usr/share/hyrax --name=hyrax opendap/hyrax:snapshot
------------------------------------------------------------------------------------------------------


What the options mean: 

---------------------------------------------------------------
-d, --detach Run container in background and print container ID
-h, --hostname Container host name
-p, --publish Publish a container's port(s) to the host
-v, --volume Bind mount a volume
--name Assign a name to the container
---------------------------------------------------------------

This command will fetch the container *opendap/hyrax:snapshot* from Docker Hub. Thw _snapshot_ is the latest build of the container. It will then _run_ the container and return the container ID. The _hyrax_ server is now running on you computer and can be accessed with a web browser, curl, etc. More on that in a bit.

The volume mount, from `$HDF4_DIR` to `'/usr/share/hyrax'` mounts the current directory of the host computer running the container to the directory _/usr/share/hyrax_ inside the container. That directory is the root of the server's data tree. This means that the HDF4 files you copied into the `HDF4_DIR` directory will be accessible by the server running in the container. That will be useful for testing later on.

Note: If you want to use a specific container version, just substitute the version info for _snapshot._

Check that the container is running using:

----------
 docker ps
----------

This will show a somewhat hard-to-read bit of information about all the running Docker container on you host:

------------------------------------------------------------------------------------------------------------------------------
CONTAINER ID        IMAGE                COMMAND              CREATED          STATUS            PORTS                    NAMES
2949d4101df4   opendap/hyrax:snapshot   "/entrypoint.sh -"   15 seconds ago   Up 14 seconds   8009/tcp, 8443/tcp, 
10022/tcp, 11002/tcp, 0.0.0.0:8080->8080/tcp   hyrax
------------------------------------------------------------------------------------------------------------------------------

If you want to stop the containers, use

---------------------------
docker rm -f <CONTAINER ID>
---------------------------

where the `<CONTAINER ID>` for the one we just started and shown in the output of _docker ps -a_ above is _2949d4101df4_. No need to stop the container now, I'm just pointing out how to do it because it's often useful.


==== Running the {DMRpp} builder ====

NOTE: At the end of this, I'll include a shell script that takes away many of these steps, but the script obscures some aspects of the command that you might want to tweak, so the following shows you all the details. Skip to *Simple shell command* to skip over these details.

Make sure you are in the directory with the HDF4 files for these steps. 

Get the command to return its help information:

-------------------------------------
docker exec -it hyrax get_dmrpp_h4 -h
-------------------------------------


will return:

-------------------------------------------------------------------------
usage: get_dmrpp_h4 [-h] -i I [-c CONF] [-s] [-u DATA_URL] [-D] [-v]

Build a dmrpp file for an HDF4 file. get_dmrpp_h4 -i h4_file_name. A dmrpp
file that uses the HDF4 file name will be generated.

optional arguments:
  
...
-------------------------------------------------------------------------

Let's build a {DMRpp} now, by explicitly using the container:

--------------------------
docker exec -it hyrax bash
--------------------------

starts the _bash_ shell in the container, with the current directory as root (/)

---------------
[root@hyrax /]# 
---------------


Change to the directory that is the root of the data (you'll see your HDF4 files in here):


--------------------
 cd /usr/share/hyrax
--------------------


You will see, roughly:


-----------------------------------
[root@hyrax /]# cd /usr/share/hyrax
[root@hyrax hyrax]# ls
3B42.19980101.00.7.HDF
3B42.19980101.03.7.HDF
3B42.19980101.06.7.HDF

...
-----------------------------------


In that directory, use the _get_dmrpp_h4_ command to build a {DMRpp} document for one of the files:

--------------------------------------------------------------------------------------------------------------
[root@hyrax hyrax]# get_dmrpp_h4 -i 3B42.20130111.09.7.HDF -u 'file:///usr/share/hyrax/3B42.20130111.09.7.HDF'
--------------------------------------------------------------------------------------------------------------

Copy that pattern for whatever file you use. From the `/usr/share/hyrax` directory, you pass _get_dmrpp_h4_ the name of the file (because it's local to the current directory) using the *-i* option. The *-u* option tells the command to embed the URL that follows it in the {DMRpp}. I've used a _file://_  URL to the file _/usr/share/hyrax/3B42.19980101.00.7.HDF_. 


NOTE: In the URL above, three slashes following the colon: two from the way a URL names a protocol and one because the pathname starts at the root directory.

Building the {DMRpp} and embedding a _file://_ URL will enable testing the {DMRpp}.


==== Using the server to examine data returned by the {DMRpp} ====


Let's look at how the _hyrax_ service will treat that data file using the {DMRpp}. In a browser, go to  http://localhost:8080/opendap/[http://localhost:8080/opendap/]

.Hyrax Catalog view of all files available.
image::Hyrax-including-new-DMRpp.png[width=650, height=400]


NOTE: _The server caches data catalog information for 5 minutes (although this can be configured) so new items (e.g., {DMRpp} documents) may not show up right away. To force the display of a {DMRpp} that you just created, click on the source data file name and edit the URL so that the suffix *.dmr.html* is replaced by *.dmrpp/dmr* ._


Click on your equivalent of the *3B42.20130111.09.7.HDF* link, subset, download and open in Panoply or the equivalent.

.Page view of the DAP _Data Request Form_ for subsetting the dataset.
image::Hyrax-subsetting.png[width=650, height=400]

You can run batch tests in lots of files by building many DMR++ documents and then asking the server for various responses (_nc4_, _dap_) from the {DMRpp} and the original file. Those could be compared using various schemes, although in its entirety that is beyond this section's scope, the command _getdap4_ is also included in the container and could be used to compare _dap_ responses from the data file and the {DMRpp} document.

Below is a comparison of the same underlying data, the left window shows the data returned using the {DMRpp}, the right shows the data read directly from the file using the server's builtin HDF4 reader. 


.Comparison of responses from a {DMRpp} and the native file handler.
image::Data-comparison.png[width=650, height=400]


==== Simple shell command ====

Here is a simple shell command that you can run on the host computer that will eliminate most of the above. 

NOTE: ''In the spirit of a recipe, I'll restate the earlier command for starting the docker container with the *get_dmrpp_h4* command and the *hyrax* server.''

Start the container:

-----------------------------------------------------------------------------------------------------
docker run -d -h hyrax -p 8080:8080 -v $HDF4_DIR:/usr/share/hyrax --name=hyrax opendap/hyrax:snapshot
-----------------------------------------------------------------------------------------------------

Check if it is running:

---------
docker ps
---------

The command, written for the Bourne Shell, is:

----------------------------------------------
#!/bin/sh
#
# usage get_dmrpp_h4.sh <file>

data_root=/usr/share/hyrax

cat <<EOF | docker exec --interactive hyrax sh
cd $data_root
get_dmrpp_h4 -i $1 -u "file://$data_root/$1"
EOF
----------------------------------------------

Copy that, save it in a file (I named the file _get_dmrpp_h4.sh_).

Run the command on the host (not the docker container) and in the directory with the HDF4 files (you don't have to do that, but sorting out the details is left as an exercise for the reader. Run the command like this: 

--------------------------------------------------------
 ./get_dmrpp_h4.sh AMSR_E_L3_SeaIce25km_V15_20020601.hdf
--------------------------------------------------------


The {DMRpp} will appear when the command completes. 

---------------------------------------------------------------------------------------------
(hyrax500) hyrax_git/HDF4-dir % ls -l
total 1251240
-rw-r--r--@ 1 jimg  staff    1250778 Aug 22 22:31 AMSR_E_L2_Land_V09_200206191112_A.hdf
-rw-r--r--@ 1 jimg  staff   20746207 Aug 22 22:32 AMSR_E_L3_SeaIce25km_V15_20020601.hdf
-rw-r--r--  1 jimg  staff    3378674 Aug 28 17:37 AMSR_E_L3_SeaIce25km_V15_20020601.hdf.dmrpp
---------------------------------------------------------------------------------------------


== Building {DMRpp} files for HDF5/NetCDF4 with _get_dmrpp_ ==



The application that builds the {DMRpp} files is a command line tool called _get_dmrpp_. It in turn utilizes other executables such as _build_dmrpp_, _reduce_mdf_, _merge_dmrpp_ (which rely in turn on the _HDF5_handler_ and the HDF5 library), along with a number of UNIX shell commands.

All of these components are install with each recent version of the Hyrax Data Server

You can see the _get_dmrpp_ usage statement with the command:

------------
get_dmrpp -h
------------


=== Using _get_dmrpp_ ===

The way that _get_dmrpp_ is invoked controls the way that the data are ultimately represented in the resulting {DMRpp} file(s). 

The _get_dmrpp_ application utilizes software from the Hyrax data server to produce the base DMR document which is used to construct the {DMRpp} file. 

The Hyrax server has a long list of configuration options, several of which can substantially alter the structural and semantic representation of the dataset as seen in the {DMRpp} files generated using these options.

=== Command line options ===

The command line switches provide a way to control the output of the tool. In addition to common options like verbose output or testing modes, the tool provides options to build extra (aka 'sidecar') data files that hold information needed for CF compliance if the original HDF5 data files lack that information (see the ''missing data'' section ). In addition, it is often desirable to build {DMRpp} files before the source data files are uploaded to a cloud store like S3. In this case, the URL to the data may not be known when the {DMRpp} is built. We support this by using placeholder/template strings in the ''dmr++'' and which can then be replaced with the URL at runtime, when the {DMRpp} file is evaluated. See the '-u' and '-p' options below.


==== Inputs ====


*-b* ::
	The fully qualified path to the top level data directory. Data files read by _get_dmrpp_ must be in the directory tree rooted at this location and their names expressed as a path relative to this location. The value may not be set to `/` , or `/etc`. The default value is `/tmp` if a value is not provided. All the data files to be processed must be in this directory or one of its subdirectories. If _get_dmrpp_ is being executed from same directory as the data then `-b `pwd`` or `-b .` works as well.

*-u* ::
	This option is used to specify the location of the binary data object. It’s value must be a http, https, or a `file://` URL. This URL will be injected into the {DMRpp} when it is constructed. If option `-u` is not used; then the template string `OPeNDAP_DMRpp_DATA_ACCESS_URL` will be used and the {DMRpp} will substitute a value at runtime.

*-c* ::
	The path to an alternate bes configuration file to use.

*-s* ::
	The path to an optional addendum configuration file which will be appended to the default BES configuration. Much like the `site.conf` file works for the full server deployment it will be loaded last and the settings there-in will have an override effect on the default configuration.


==== Output ====

*-o* ::
	The name of the file to create.

==== Verbose Output Modes ====

*-h* ::
	Show help/usage page.
*-v* ::
	verbose mode, prints the intermediate DMR.
*-V* ::
	Very verbose mode, prints the DMR, the command, and the configuration file used to build the DMR.
*-D* ::
	Just print the DMR that will be used to build the {DMRpp}.
*-X* ::
	Do not remove temporary files. May be used independently of the `-v` and/or `-V` options.


==== Tests ====

*-T* ::
	Run ALL hyrax tests on the resulting {DMRpp} file and compare the responses the ones generated by the source HDF5 file.
*-I* ::
	Run hyrax inventory tests on the resulting {DMRpp} file and compare the responses the ones generated by the source HDF5 file.
*-F* ::
	Run hyrax value probe tests on the resulting {DMRpp} file and compare the responses the ones generated by the source HDF5 file.

==== Missing Data Creation ====


*-M* ::
	Build a 'sidecar' file that holds missing information needed for CF compliance (e.g., Latitude, Longitude and Time coordinate data).
*-p* ::
	Provide the URL for the Missing data sidecar file. If this is not given (but -M is), then a template value is used in the {DMRpp} file and a real URL is substituted at runtime.
*-r* ::
	The path to the file that contains missing variable information for sets of input data files that share common missing variables. The file will be created if it doesn't exist and the result may be used in subsequent invocations of _get_dmrpp_ (using `-r`) to identify the missing variable file.


==== AWS Integration ====
The _get_dmrpp_ application supports both S3 hosted granules as inputs, and uploading generated {DMRpp} files to an S3 bucket.

*S3 Hosted granules are supported by default* ::
	When the `get_dmrpp` application sees that the name of the input file is an S3 URL it will check to see if the AWS CLI is configured and if so `get_dmrpp` will attempt retrieve the granule and make a {DMRpp} utilizing whatever other options have been chosen. **For example:**
	
	get_dmrpp -b `pwd` s3://bucket_name/granule_object_id


*-U* ::
	The `-U` command line parameter for `get_dmrpp` instructs `get_dmrpp` application to upload the generated {DMRpp} file to S3, but only when the following conditions are met:
	- The name of the input file is an S3 URL.
	- The `AWS CLI` has been configured with credentials that provide `r+w` permissions for the bucket referenced in the input file S3 URL.
	- The `-U` option has been specified.
	If all three of the above are true then `get_dmrpp` will copy the retrieve the granule, create a {DMRpp} file from the granule, and copy the resulting {DMRpp} file (as defined by the `-o` option) to the source S3 bucket using the well known NGAP sidecar file naming convention: *s3://bucket_name/granule_object_id.dmrpp*.  For example:
	
	get_dmrpp -U -o foo -b `pwd` s3://bucket_name/granule_object_id


=== _HDF5_handler_ Configuration ===

Because _get_dmrpp_ uses the _HDF5_handler_ software to build the {DMRpp} the software must inject the _HDF5_handler_'s configuration. 

The default configuration is large, but any valued may be altered at runtime.


Here are some of the commonly manipulated configuration parameters with their default values:

----------------------------------
 H5.EnableCF=true
 H5.EnableDMR64bitInt=true
 H5.DefaultHandleDimension=true
 H5.KeepVarLeadingUnderscore=false
 H5.EnableCheckNameClashing=true
 H5.EnableAddPathAttrs=true
 H5.EnableDropLongString=true
 H5.DisableStructMetaAttr=true
 H5.EnableFillValueCheck=true
 H5.CheckIgnoreObj=false
----------------------------------

// NOTE: Mikejmnez. It states here that H5.EnableCF is `true` by default. But below it states that it is `false` by default...

==== Note to DAACs with existing Hyrax deployments. ====

If your group is already serving data with Hyrax and the data representations that are generated by your Hyrax server are satisfactory, then a careful inspection of the localized configuration, typically held in `/etc/bes/site.conf`, will help you determine what configuration state you may need to inject into _get_dmrpp_.

=== The _H5.EnableCF_ option ===

Of particular importance is the _H5.EnableCF_ option, which instructs the _get_dmrpp_ tool to produce https://cfconventions.org/[Climate Forecast convention (CF)] compatible output based on metadata found in the granule file being processed. 

Changing the value of _H5.EnableCF_ from *false* to *true* will have (at least) two significant effects.

It will:

- Cause _get_dmrpp_ to attempt to make the dmr++ metadata CF compliant.
- Remove Group hierarchies (if any) in the underlying data granule by flattening the Group hierarchy into the variable names.  

By default _get_dmrpp_ the _H5.EnableCF_ option is set to false:

--------------------
 H5.EnableCF = false
--------------------


There is a much more comprehensive discussion of this key feature, and others, in the https://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#HDF5-handler[HDF5 Handler section] of the Appendix in the Hyrax Data Server Installation and Configuration Guide.

=== Missing data, the CF conventions and _HDF5_ ===

Many of the _HDF5_ files produced by NASA and others do not contain the domain coordinate data (such as latitude, longitude, time, etc.) as a collection of explicit values. Instead, information contained in the dataset metadata can be used to reproduce these values.

In order for a dataset to be Climate Forecast (CF) compatible it must contain these domain coordinate data values.

The Hyrax _HDF5_handler_ software, utilized by the _get_dmrpp_ application, can create this data from the dataset metadata.  The _get_dmrpp_ application places these generated data in a “sidecar” file for deployment with the source _HDF5/netcdf-4_ file.

// **jhrg** Content editing resumes jhrg 4/19/25
== _BETA_ Building {DMRpp} files HDF5, NetCDF4, HDF4, and HDF4-EOS2 ==
[NOTE]
This is a command line tool for building {DMRpp} documents was introduced in March 2025 and will initially be available only using the Hyrax Docker container version _1.17.1-126_ or later.

There is now one command that can be used to build {DMRpp} documents for both HDF5, netCDF4, HDF4, and HDF-EOS2. This command is called `gen_dmrpp_side_car`, so named because will not only build the {DMRpp} but also any additional _sidecar_ data file that needed for efficient and user-friendly access to data in the original source file. This new {DMRpp} builder will generate those geo-referencing values and store then in an additional sidecar file. The Hyrax server can efficiently read those values just as it reads values from the original data file.

// In the following, we outline using the _Hyrax_ Docker container to run this command because doing so has the added benefit that you can test the {DMRpp} that has just been built. While not suitable for production processes, this is useful when working with a new collection of data to ensure the system is returning the correct values. For production runs where many data files will be processed, it is better to use the smaller _BES_ Docker container. The process for using that is the same as for the Hyrax container, except that testing the results is more involved and depends on the configuration of the data processing system.

=== Summary of the {DMRpp} Build Process
==== Assumptions
* Docker installed on your computer and at least a basic understanding of its use.
* data files in a directory on your computer

In the following, `%` is the terminal prompt. Only some commands produce output, and for those that do, the output is shown below the command. The paths, etc., on your computer will almost certainly be different.

==== Start
Change to the directory that holds your data files and assign an environment variable to the full pathname of that directory to streamline some of the later steps. In my case that directory is called `HDF4-dir`.
[source,sh]
----
% cd HDF4-dir
% export DATA=$(pwd)
% echo $DATA
/Users/jimg/src/opendap/hyrax_git/HDF4-dir
----
Run the Docker container. The command returns The docker run command returns the Container ID (a long hexadecimal string) when the `-d` (run a detached container) is used. The `--name` option sets _hyrax_ as the name of the container which will be used in later commands. Running the container this way enables us to use both build {DMRpp} documents and test them, the latter using the Hyrax server that is part of the container.
[source,sh]
----
% docker run -d -h hyrax -p 8080:8080 -v $DATA:/usr/share/hyrax --name=hyrax opendap/hyrax:1.17.1-126
9c88a0d4abe55f17802afd81150280073314f3940b9cd4973ea60dbc43f733a9
----
Here are the files on my computer in the directory assigned to $DATA
[source,sh]
----
% ls
3B42.19980101.00.7.HDF
3B42.19980101.03.7.HDF
3B42.19980101.06.7.HDF
3B42.19980101.09.7.HDF
3B42.20130111.06.7.HDF
3B42.20130111.09.7.HDF
AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf
AIRS.2009.01.02.L3.RetStd_IR001.v7.0.3.0.G20160024358.hdf
AIRS.2009.01.03.L3.RetStd_IR001.v7.0.3.0.G20160024538.hdf
AMSR_E_L2_Land_V09_200206191023_D.hdf
AMSR_E_L2_Land_V09_200206191112_A.hdf
AMSR_E_L3_SeaIce25km_V15_20020601.hdf
MCD12Q1.A2022001.h10v06.061.2023243073808.hdf
MCD19A1.A2024025.h10v06.061.2024027100206.hdf
MOD10A1F.A2024025.h01v08.061.2024027134335.hdf
MOD10A1F.A2024025.h01v09.061.2024027130238.hdf
MOD10A1F.A2024025.h01v10.061.2024027131939.hdf
MOD11A1.A2024025.h10v06.061.2024028004317.hdf
----
To build a {DMRpp} for the first AIRS file (`AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf`) we can run the gen_dmrpp_side_car command using exec using the file's name.
[source,sh]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf -H -U
% ls
3B42.19980101.00.7.HDF
3B42.19980101.03.7.HDF
3B42.19980101.06.7.HDF
3B42.19980101.09.7.HDF
3B42.20130111.06.7.HDF
3B42.20130111.09.7.HDF
AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf
AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf.dmrpp
AIRS.2009.01.02.L3.RetStd_IR001.v7.0.3.0.G20160024358.hdf
...
----
Another example, this time both the {DMRpp} and a sidecar _missing data_ file (`3B42.19980101.00.7.HDF_mvs.h5`) were built. Even though the input data file was an HDF4 file, the missing data file uses HDF5 to store the values. For this example, we should the sizes of the input data file and the smaller {DMRpp} and missing data file, which together are only 2% of the data file's size.
[source,sh]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i 3B42.19980101.00.7.HDF -H -U
% ls -l
total 1245840
-rw-r--r--@ 1 jimg  staff     774595 Aug 22  2024 3B42.19980101.00.7.HDF
-rw-r--r--  1 jimg  staff       6514 Apr 21 22:42 3B42.19980101.00.7.HDF.dmrpp
-rw-r--r--  1 jimg  staff       8075 Apr 21 22:42 3B42.19980101.00.7.HDF_mvs.h5
-rw-r--r--@ 1 jimg  staff     765742 Aug 22  2024 3B42.19980101.03.7.HDF
...
----



Here's a quick summary of what will follow. First, the _Hyrax_ Docker container will be downloaded from the OPeNDAP Docker Hub repository. Then  a directory will be made where the data files (the input files) will be stored. The Docker container will be run so that directory is mounted by the container and its contents accessed by the Hyrax data server and gen_dmrpp_side_car program. The program will make the {DMRpp}. The program will also make a sidecar file to hold the georeferencing information it that's necessary. Because the contents of the directory are shared by both the container and the host computer, it will be simple to use the {DMRpp} and sidecar files.

=== Detailed Steps
[NOTE]
These steps assume that you have the Docker daemon and other software on your computer.

==== Get and Run the Docker Container
First, get the version `1.17.1-126` of the Hyrax Docker container. This will download that version of the  Docker container to your local computer.
[source,sh]
----
% docker pull opendap/hyrax:1.17.1-126
----
Next, make a directory where data files can be stored. For this example, the current working directory (CWD) will be used. The full path to the CWD will be stored in an environment variable named `HDF4_DIR`. You can choose any name, and it need not be the CWD, but setting the environment variable and making it the CWD will make the explanation below easier to follow.

[source,sh]
----
% export HDF4_DIR=$(pwd)
----

Run the Docker container :

[source,sh]
----
% docker run -d -h hyrax -p 8080:8080 -v $HDF4_DIR:/usr/share/hyrax --name=hyrax opendap/hyrax:1.17.1-126
----

What the options mean:
[horizontal]
-d, --detach:: Run container in the background and print container ID
-h, --hostname:: Set the container's host name
-p, --publish:: Publish a container's port(s) to the Docker host
-v, --volume:: Mount a volume so that the container can use files on the Docker host
--name:: Assign a name to the container; this name can be used in later Docker commands

[NOTE]
This command will download the Hyrax Docker container if you did not already do so.

The `docker run -d ...` command will run the Hyrax container on your computer (called the _host_ computer) in _detached_ mode. The Hyrax container hold both the complete Hyrax service and the `gen_dmrpp_side_car` command. Later this server will be used to test the {DMRpp} documents that are built.

The volume mount, from `$HDF4_DIR` to `/usr/share/hyrax` mounts the current directory of the host computer running the container to the directory _/usr/share/hyrax_ inside the container. That directory is the root of the server's data tree. This means that the HDF4 files you copied into the `HDF4_DIR` directory will be accessible by the server running in the container. That will be useful for testing later on.

[NOTE]
If you want to use a specific container version, substitute the version info for _1.17.1-126_ in the above commands. For example, to use the latest build of the container, use _snapshot_ instead ot the version number.

Check that the container is running using:
[source,sh]
----
% docker ps
----
or make a command alais for a more compact listing than the default output of `docker ps`
[source,sh]
----
% alias d-ps='docker ps --format "table {{.ID}}\t{{.Names}}\t{{.Status}}\t{{.Image}}"'
----
This will show a somewhat easier-to-read bit of information about all the running Docker container on your host:
[source,sh]
----
% d-ps

CCONTAINER ID   NAMES     STATUS          IMAGE
82074fe6ccfe    hyrax     Up 13 minutes   opendap/hyrax:1.17.1-126
----
If you want to stop the container, use
[source,sh]
----
% docker rm -f hyrax
----
No need to stop the container now, we're pointing out how to do it because it's often useful.

==== Running the {DMRpp} builder ====
// TODO is this still true jhrg 4/21/25
NOTE: At the end of this, I'll include a shell script that takes away many of these steps. However, the script obscures some aspects of the command that you might want to tweak, so the following shows you all the details. Skip to *Simple shell command* to skip over these details.

We can use the Docker command `exec` to run the `gen_dmrpp_side_car` program inside the already running Docker container. For example, we can look at the command's online help using the `-h` option of the command.
Get the command to return its help information:

[source,sh]
----
% docker exec -it hyrax gen_dmrpp_side_car -h

usage: gen_dmrpp_side_car [-h] -i I [-c] [-H] [-u URL] [-D] [-U] [-s SURL]
                          [-S ESURL] [-v]

Generate the dmrpp file for an HDF4/HDF5 file. If having the missing data, the
HDF5 side car file that stores the data is also generated.

optional arguments:
  -h, --help            show this help message and exit
  -i I                  The HDF4/5 file must be provided under the current
                        directory or its children directories.
  -c                    For the netCDF4/HDF5 files, the dmrpp and the side car
                        files will be generated via the EnableCF option.
  -H, --HDF4            Generate dmrpp from an HDF4 file.
  -u URL, --URL URL     The URL that provides the HDF4/5 file path(including
                        the file name at the end).
  -D, --DisableSideCar  The sidecar file is not generated for the HDF4 file.
  -U, --SCTURL          the location of the sidecar file will contain the
                        template string OPeNDAP_DMRpp_SC_DATA_ACCESS_URL,
                        which can be replaced at runtime.
  -s SURL, --SURL SURL  The URL that provides the location of the side car
                        file.
  -S ESURL, --ESURL ESURL
                        The URL that provides the existing side car file
                        path(must be absolute path or under current dir). The
                        dmrpp file of the side car file should also exist.
  -v, --verbosity       Detailed description message and other information.

----

Let's build a {DMRpp} now, by explicitly using the container:

--------------------------
docker exec -it hyrax bash
--------------------------

starts the _bash_ shell in the container, with the current directory as root (/)

---------------
[root@hyrax /]#
---------------


Change to the directory that is the root of the data (you'll see your HDF4 files in here):


--------------------
 cd /usr/share/hyrax
--------------------


You will see, roughly:


-----------------------------------
[root@hyrax /]# cd /usr/share/hyrax
[root@hyrax hyrax]# ls
3B42.19980101.00.7.HDF
3B42.19980101.03.7.HDF
3B42.19980101.06.7.HDF

...
-----------------------------------


In that directory, use the _get_dmrpp_h4_ command to build a {DMRpp} document for one of the files:

--------------------------------------------------------------------------------------------------------------
[root@hyrax hyrax]# get_dmrpp_h4 -i 3B42.20130111.09.7.HDF -u 'file:///usr/share/hyrax/3B42.20130111.09.7.HDF'
--------------------------------------------------------------------------------------------------------------

Copy that pattern for whatever file you use. From the `/usr/share/hyrax` directory, you pass _get_dmrpp_h4_ the name of the file (because it's local to the current directory) using the *-i* option. The *-u* option tells the command to embed the URL that follows it in the {DMRpp}. I've used a _file://_  URL to the file _/usr/share/hyrax/3B42.19980101.00.7.HDF_.


NOTE: In the URL above, three slashes follow the colon: two from the way a URL names a protocol and one because the pathname starts at the root directory.

Building the {DMRpp} and embedding a _file://_ URL will enable testing the {DMRpp}.


==== Using the server to examine data returned by the {DMRpp} ====


Let's look at how the _hyrax_ service will treat that data file using the {DMRpp}. In a browser, go to  http://localhost:8080/opendap/[http://localhost:8080/opendap/]

.Hyrax Catalog view of all files available.
image::Hyrax-including-new-DMRpp.png[width=650, height=400]


NOTE: _The server caches data catalog information for 5 minutes (although this can be configured) so new items (e.g., {DMRpp} documents) may not show up right away. To force the display of a {DMRpp} that you just created, click on the source data file name and edit the URL so that the suffix *.dmr.html* is replaced by *.dmrpp/dmr* ._


Click on your equivalent of the *3B42.20130111.09.7.HDF* link, subset, download and open in Panoply or the equivalent.

.Page view of the DAP _Data Request Form_ for subsetting the dataset.
image::Hyrax-subsetting.png[width=650, height=400]

You can run batch tests in lots of files by building many DMR++ documents and then asking the server for various responses (_nc4_, _dap_) from the {DMRpp} and the original file. Those could be compared using various schemes, although in its entirety that is beyond this section's scope, the command _getdap4_ is also included in the container and could be used to compare _dap_ responses from the data file and the {DMRpp} document.

Below is a comparison of the same underlying data, the left window shows the data returned using the {DMRpp}, the right shows the data read directly from the file using the server's builtin HDF4 reader.


.Comparison of responses from a {DMRpp} and the native file handler.
image::Data-comparison.png[width=650, height=400]


// Original text resumes jhrg 4/19/25

=== HDF4 
To generate a dmrpp file for the HDF4 file hdf4.hdf. Do the following:
[source,sh]
----
gen_dmrpp_side_car -I hdf4.hdf -H -U
----
If a sidecar file is generated, the sidecar file is always named after the original HDF4 file plus `_mvs.h5`. For example, `hdf4.hdf_mvs.h5.`

NOTE: Note: `-H -U` are critical and cannot be omitted.

=== HDF5
To generate a dmrpp file for the HDF5 file `HDF5.h5`. Do the following:
[source,sh]
----
gen_dmrpp_side_car -i HDF5.h5  -U
----


== Hyrax - Serving data using {DMRpp} files ==

There are three fundamental deployment scenarios for using {DMRpp} files to serve data with the Hyrax data server.

This can be simple categorized as follows:
The {DMRpp} file(s) are XML files that contain a root `dap4:Dataset` element with a `dmrpp:href` attribute whose value is one of:

. A http(s):// URL referencing to the underlying granule files via http.

. A file:// URL that references the granule file on the local filesystem in a location that is inside the BES' data root tree.

. The template string `OPeNDAP_DMRpp_DATA_ACCESS_URL`

Each will be discussed in turn below.

NOTE: By default, Hyrax will automatically associate files whose name ends with ".dmrpp" with the *{DMRpp}* handler.


=== Using {DMRpp} with http(s) URLs ===

If the {DMRpp} files that you wish to serve contain `dmrpp:href` attributes whose values are http(s) URLs then there are 2+1 steps to serve the data:

. Place the {DMRpp} files on the local disk inside the directory tree identified by the `BES.Catalog.catalog.RootDirectory` in the BES configuration.
. Ensure that the Hyrax `AllowedHosts` list is configured to allow Hyrax to access those target URLs. This can be accomplished by adding new regex records to the `AllowedHosts` list in `/etc/bes/site.conf`, creating that file as need be.
. If the data URLs require authentication to access then you'll need to configure Hyrax for that too.


=== Using {DMRpp} with file URLs ===

Using {DMRpp} files with locally held files can be useful for verifying that {DMRpp} functionality is working without relying on network access that may have data rate limits, authenticated access configuration, or security access constraints. Additionally, in many cases the {DMRpp} access to the locally held data may be significantly faster than through the native `netcdf-4/HDF5` data handlers.

In order to use {DMRpp} files that contain file:// URLs:
. Place the {DMRpp} files on the local disk inside the directory tree identified by the `BES.Catalog.catalog.RootDirectory` in the BES configuration.
. Ensure that the {DMRpp} files contain only file:// URLs that refer to data granule files that are inside the directory tree identified by the `BES.Catalog.catalog.RootDirectory` in the BES configuration.

Note: For Hyrax, a correctly formatted file URL must start with the protocol `file://` followed by the full qualified path to the data granule, for example: 

`/usr/share/hyrax/ghrsst/some_granule.h5`

so that the completed URL will have three slashes after the first colon:

`file:///usr/share/hyrax/ghrsst/some_granule.h5`

=== Using {DMRpp} with the template string (NASA). ===

Another way to serve {DMRpp} files with Hyrax is to build the {DMRpp} files *without* valid URLs but with a template string that is replaced at runtime. If no target URL is supplied to _get_drmpp_ at the time that the {DMRpp} is generated the template string: `*OPeNDAP_DMRpp_DATA_ACCESS_URL*` will be added to the file in place of the URL. The at runtime it can be replaced with the correct value.

Currently, the only implementation of this is Hyrax's NGAP service which, when deployed in the NASA NGAP cloud, will accept "restified path" URLs that are defined as having a URL path component with two mandatory and one optional parameters:

----------------------------------------------------
 MANDATORY: "/collections/UMM-C:{concept-id}"
 OPTIONAL:  "/UMM-C:{ShortName} '.' UMM-C:{Version}"
 MANDATORY: "/granules/UMM-G:{GranuleUR}"
----------------------------------------------------

*Example:* https://opendap.earthdata.nasa.gov/collections/C1443727145-LAADS/MOD08_D3.v6.1/granules/MOD08_D3.A2020308.061.2020309092644.hdf.nc

When encountering this type of URL Hyrax will decompose it and use the content to formulate a query to the NASA CMR in order to retrieve the data access URL for the granule and for the {DMRpp} file. It then retrieves the {DMRpp} file and injects the data URL so that data access can proceed as described above.


More on the Restified Path can be found https://wiki.earthdata.nasa.gov/display/DUTRAIN/Feature+analysis%3A+Restified+URL+for+OPENDAP+Data+Access[here] ([.underline]#NOTE: You need the right permissions access the previous URL#).

== Recipe: Building and testing {DMRpp} files ==
There are two recipes shown here, the first using a Hyrax docker container and a second using the container that is part of the NASA EOSDIS Cumulus task.

*_Prerequisites_*:

- The Docker daemon running on a system that also supports a shell (the examples use bash in this section).

=== Recipe: Building {DMRpp} files using a Hyrax docker container ===

. Acquire representative granule files for the collection you wish to import. Put them on the system that is running the Docker daemon. For this recipe we will assume that these files have been placed in the directory:

	/tmp/dmrpp

. Get the most up-to-date Hyrax docker image:

	docker pull opendap/hyrax:snapshot

. Start the docker container, mounting your data directory on to the docker image at `/usr/share/hyrax`:

	docker run -d -h hyrax -p 8080:8080 --volume /tmp/dmrpp:/usr/share/hyrax --name=hyrax opendap/hyrax:snapshot

. Get a first view of your data using `get_dmrpp` with its default configuration.

.. If you want you can build a {DMRpp} for an example "input_file" using a docker exec command:

	docker exec -it hyrax get_dmrpp -b /usr/share/hyrax -o /usr/share/hyrax/input_file.dmrpp -u "file:///usr/share/hyrax/input_file" "input_file"

.. Or if you want more scripting flexibility you can log in to the docker container to do the same:

... Login to the docker container:

	docker exec -it hyrax /bin/bash

... Change working dir to data dir: 

	cd /usr/share/hyrax

... Set the data directory to the current one (`-b $(pwd)`) and set the data URL (`-u`) to the fully qualified path to the input file.

	get_dmrpp -b $(pwd) -o foo.dmrpp -u "file://"$(pwd)"/your_test_file" "your_test_file"

NOTE: Now that you have made a dmr++ file, use the running Hyrax server to view and test it by pointing your browser at: http://localhost:8080/opendap/

[start=5]
. You can also batch process all of your test granules, if you want to go that route. The following script assumes your source data files end with '.h5'.

NOTE: The resulting *{DMRpp}* files should contain the correct file:// URLs and be correctly located so that they may be tested with the Hyrax service running in the docker instance.

------------------------------------------------------------------------------------
#!/bin/bash
# This script will write each output file as a sidecar file into 
# the same directory as its associated input granule data file.

# The target directory to search for data files 
target_dir=/usr/share/hyrax
echo "target_dir: $target_dir";

# Search the target_dir for names matching the regex \*.h5 
for infile in `find "$target_dir" -name \*.h5`
do
    echo " Processing: $infile"

    infile_base=`basename "${infile}"`
    echo "infile_base: $infile_base"

    bes_dir=`dirname "${infile}"`
    echo "    bes_dir: $bes_dir"

    outfile="$infile.dmrpp"
    echo "     Output: $outfile"

    get_dmrpp -b "$bes_dir" -o "$outfile" -u "file://$infile" "$infile_base"
done
------------------------------------------------------------------------------------

TIP: Remember that you can use the Hyrax server that is running in the docker container to view and test the {DMRpp} files you just created by pointing your browser at: http://localhost:8080/opendap/


=== Testing and qualifying {DMRpp} files ===
In the previous section/step we created some initial {DMRpp} files using the default configuration. It is crucial to make sure that they provide the representation of the data that you and your users are expecting, and that they will work correctly with the Hyrax server. (See the following sections for details). If the generated {DMRpp} files do not match expectations then the default configuration of the `get_dmrpp` may need to be amended using the `-s` parameter.
If the data are currently being served by your DAAC's on-prem team this is where understanding exactly what the localizations made to the configurations of the on-prem Hyrax instances deployed for the collection is important. These localization will probably need to be injected into `get_drmpp` in order to produce the correct data representation in the {DMRpp} files.


=== Flattening Groups ===
By default `get_dmrpp` will preserve and show group hierarchies. If this is not desired, say for CF-1.0 compatibility, then you can change this by creating a small amendment to `get_dmrpp`'s default configuration. 

First create the amending configuration file:

	echo "H5.EnableCF=true" > site.conf

Then, change the invocation of `get_dmrpp` in the above example by adding the `-s` switch:

	get_dmrpp -s site.conf -b `pwd` -o "$dmrpp_file" -u "file://"`pwd`"/$file" "$file"

And re-run the {DMRpp} production as shown above.



=== DAP representations ===
We have test and assurance procedures for DAP4 and DAP2 protocols below. Both are important. For legacy datasets the DAP2 request API is widely used by an existing client base and should continue to be supported. Since DAP4 subsumes DAP2 (but with somewhat different API semantics) It should be checked for legacy datasets as well. For more modern datasets that content DAP4 types such as Int64 that are not part of the DAP2 specification or implementations we will need to rely on eliding the instances of unmapped types, or return an error when this is encountered.


------------------------------------------------------
# Test Constants:
GRANULE_FILE="some_name.h5"
# Granule URL
gf_url="http://localhost:8080/opendap/$GRANULE_FILE"
------------------------------------------------------



==== Inspect the {DMRpp} files ====

Do the {DMRpp} files have the expected `dmrpp:href` URL(s)?

	head -2 "$GRANULE_FILE.dmrpp"

==== Check DAP4 DMR Response ====
Inspect `$gf_url.dmrpp.dmr`

. Get the document, save as `foo.dmr`:

	curl -L -o foo.dmr "$gf_url.dmr"

. Is each variable's data type correct and as expected?
. Are the associated dimensions correct?


==== DAP4 Check binary data response ====

For a particular granule named GRANULE_FILE and a particular variable, named VARIABLE_NAME (Where VARIABLE_NAME is a https://opendap.github.io/dap4-specification/DAP4.html#_fully_qualified_names[full qualified DAP4 name]):

    curl -L -o dap4_subset_file "$gf_url.dap?dap4.ce=VARIABLE_NAME"
    curl -L -o dap4_subset_dmrpp "$gf_url.dmrpp.dap?dap4.ce=VARIABLE_NAME"
    cmp dap4_subset_file dap4_subset_dmrpp


==== DAP4 UI test ====

View and exercise the DAP4 Data Request Form `$gf_url.dmr.html` with a browser.

==== DAP2 Check DDS Response ====

. Inspect `$gf_url.dds`
.. Is each variable's data type correct and as expected?
.. Are the associated dimensions correct?
. Compare {DMRpp} DDS with granule file DDS -
For a particular granule named GRANULE_FILE and a particular variable named VARIABLE_NAME (Where VARIABLE_NAME is a https://zenodo.org/records/10794666[DAP2 name]):

    curl -L -o dap2_dds_file "$gf_url.dds"
    curl -L -o dap2_dds_dmrpp "$gf_url.dds"
    cmp dap2_dds_file dap2_dds_dmrpp


==== DAP2 Check binary data response ====

For a particular granule named GRANULE_FILE and a particular variable, VARIABLE_NAME (Where VARIABLE_NAME is a https://zenodo.org/records/10794666[DAP2 name]):


    curl -L -o dap2_subset_file "$gf_url.dods?VARIABLE_NAME"
    curl -L -o dap2_subset_dmrpp "$gf_url.dmrpp.dods?VARIABLE_NAME"
    cmp dap2_subset_file dap2_subset_dmrpp

NOTE: One might consider doing this with two or more variables. 

==== DAP2 UI Test ====

. View and exercise the DAP2 Data Request Form located here: `$gf_url.html`.
. Try it in Panoply! 
.. Open Panoply.
.. From the File menu select *Open Remote Dataset*...
.. Paste the `$gf_url.html` into the resulting dialog box.




