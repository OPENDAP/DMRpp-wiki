= Building and Testing DMR++ Documents
OPeNDAP, Inc.
{docdatetime}
:appendix-caption: Exhibit
:toc:
:toclevels: 3
:numbered:
:docinfo: shared
:icons: font
:tabsize: 4
:indent: 4
:source-highlighter: coderay
:coderay-linenums-mode: inline
:prewrap!:
:imagesdir: ./images
:homepage: www.opendap.org
:DMRpp: DMR++
:Miguel Jimenez <mjimenez@opendap.org>:
:James Gallagher <jgallagher@opendap.org>:

//image:logo-hyrax-red.svg[width=300]

////
// Pithy version of why we did this. Written by ChatGPT 4o (https://chatgpt.com/share/680fb929-06a0-8010-a492-47bd11a682bd). jhrg 4/28/25
OPeNDAP developed the DMR++ system to enable fast, efficient access to large scientific datasets without requiring full file downloads. By providing lightweight, annotated metadata, DMR++ reduces data movement and supports scalable, cloud-native workflows essential for modern research.

Some tagline versions:

DMR++: Fast, efficient, cloud-ready access to large scientific datasets.
DMR++: Smarter access to big data — without moving big files.
DMR++: Unlock scientific data faster, with lower cost and complexity.
DMR++: Lightweight metadata for powerful, scalable data access.
////
== Intended Audience
This document is for people who want to enable access to HDF and netCDF data files stored in Amazon Web Services (AWS) Simple Storage Service (S3) using the OPeNDAP _Hyrax_ data server. It describes how to build the {DMRpp} documents the Hyrax server uses. This document will also be useful to people who want to build {DMRpp} documents for other reasons, such as enabling client-side technology like _VirtualiZarr_ to access/subset remote data files  using only HTTP.

[TIP]
There are some features of the {DMRpp} builder software that are targeted specifically to the NASA Earth Science Data and Information Systems (ESDIS) project's use of S3 to store data. Look for the _TIP_ icon to find those.

== Introduction ==

The {DMRpp} is a metadata file that provides a fast and flexible way to access data stored in Amazon Web Services (AWS) Simple Storage Service (S3), or using any other service that supports HTTP.footnote:[The HTTP/S service must support the _Range_ header of HTTP/1.1. When using libcurl, both HTTP/S and the 'file:' protocols can be used.]

// Written by ChatGPT. https://chatgpt.com/share/680fb929-06a0-8010-a492-47bd11a682bd jhrg 4/2825
OPeNDAP developed the {DMRpp} system in response to the growing need for faster, more efficient access to scientific data stored in large, complex files. As datasets expanded in size and migrated to cloud-based storage systems like S3, traditional methods of reading metadata and retrieving data became increasingly cumbersome and expensive. {DMRpp} was designed to solve this problem by creating a lightweight, annotated metadata file that describes where data elements are located and how they can be accessed, without requiring full downloads or local processing. By decoupling metadata access from the underlying storage and minimizing data movement, {DMRpp} enables scalable, cloud-friendly workflows that better support the demands of modern Earth science research.

The {DMRpp} encodes the location of data residing in a binary data file/object (e.g., an https://support.hdfgroup.org/documentation/HDF5/latest/[HDF5] file) so that it can be directly accessed, without the need for an intermediate Application Programmer Interface (API) library. The binary data objects may be on a local filesystem, or they may be accessible using HTTP in, for example, an S3 bucket. The Hyrax data server from OPeNDAP can use {DMRpp} files to provide access and subsetting services for data stored in S3 without first copying their data files from S3. That is, Hyrax can use the {DMRpp} files to access and subset data 'in place.' This mirrors the behavior of the Hyrax server when used with data files stored on POSIX file systems.

The {DMRpp} is an extension of the Dataset Metadata Response (DMR) from OPeNDAP's DAP4 protocol. For a full description of DAP4 and the DMR object, see the DAP4 protocol, link:https://opendap.github.io/dap4-specification/DAP4.html[Sections 1.5.7–1.5.15]. The DMR encodes metadata information about the names, data types, and hierarchical relations of the variables that make up a dataset. The {DMRpp} adds information about the location, size, and other relevant characteristics of those variables. Software can then use this information to read binary data values directly from the dataset's file(s) without using an API library or copying the dataset to temporary storage before accessing the data.

Additional advantages to the {DMRpp} are:

. Enables data providers to take advantage of modern storage technologies for large data without having to reformat huge data collections.

. A {DMRpp} can be programmatically generated by OPeNDAP software for datasets that are made up of HDF5, NetCDF4, HDF4, and HDF4_EOS2 data files.

. Data file integrity is preserved.

[[Diagram]]
.A collection of HDF5 files in an S3 bucket. Each data file has an associated {DMRpp} file, named using the data file name with the suffix '.dmrpp'. Because the {DMRpp} uses a URL to reference the source data file, it can be stored 'close' to the data or on a different storage system.
image::DMRppS3.png[width=70%, align='center']

////
// Revised version. jhrg 4/28/25
== How Does It Work? ==

The {DMRpp} builder software reads a data file and builds a document that captures all the file's metadata, including the names, types, and associated attributes of each variable. This information is stored in a document called the Dataset Metadata Response (DMR). {DMRpp} extends the DMR by adding annotations that specify where each variable's data can be found within the file and how to decode those values. In effect, the {DMRpp} is a specially annotated DMR document.

This additional information enables:

Decoupling the annotated {DMRpp} from the physical location of the granule file.

Storing and transferring {DMRpp} files more efficiently, since they are typically much smaller than the data granules they represent.

Reading all of a file’s metadata in a single operation, instead of through the iterative processes required by many APIs.

Referencing source granules via web URLs, making the {DMRpp} file location itself independent and flexible.

Software that understands {DMRpp} content can directly access the data values held in the source granule file. It can do so without retrieving the entire file or processing it locally, even when the granule is stored in a Web Object Store like S3.

If a granule contains multiple variables and only a subset is needed, {DMRpp}-enabled software can retrieve just the bytes associated with the specified variables, further improving efficiency.

// Original version
== How Does It Work? ==
The {DMRpp} ingest software reads a data file and builds a document that holds all the file's metadata, the names and types of all the variables along with any other information bound to those variables. This information is stored in a document we call the Dataset Metadata Response (DMR). The {DMRpp} adds some extra information to this regarding where each variable can be found and how to decode those values. The {DMRpp} is simply a special annotated DMR document.

This additional information enables:

* Decoupling the annotated {DMRpp} from the location of the granule file itself.
* Since {DMRpp} files are typically significantly smaller than the source data granules they represent, they can be stored and moved for less expense.
* Reading all the file's metadata in one operation instead of the iterative process that many APIs require.
* If the {DMRpp} contains references to the source granules location on the web, the location of the {DMRpp} file itself does not matter.

Software that understands the {DMRpp} content can directly access the data values held in the source granule file. It can do so without having to retrieve the entire file and work on it locally, even when the file is stored in a Web Object Store like S3.

If the granule file contains multiple variables and only a subset of them are needed, the {DMRpp} enabled software can retrieve just the bytes associated with the specified subset(s) of desired variable(s).
////

== Supported Data Formats ==
The software to build {DMRpp} documents currently works with HDF5, netCDF4, HDF4, and HDF4-EOS2 files.footnote:[The netCDF4 format is a subset of HDF5, so HDF5 tools are used for both.] Other formats like Zarr and netCDF3 are not currently supported by the {DMRpp} software, but support could be added if requested.

=== The Gory Details ===
Technologies such as HDF5 are best characterized as tools for defining _self-describing_ data files. These files are widely adopted in scientific domains because they support a diverse range of organizational structures for information. In the case of NASA ESDIS, nearly all the more than 8,600 data collections (encompassing over one billion individual files) define distinct sets of _variables_, effectively making each collection a unique data format. Despite these differences, a small number of API libraries can be used to consistently access the data across all collections.

While we aim to provide support for all possible HDF5, HDF4, etc., data files, there are aspects of the _data models_ these API libraries implement that the current {DMRpp} software does not cover. As of April 2025, support for HDF5, as it is used by the NASA ESDIS collections, is close to complete. The best approach to determining if the OPeNDAP {DMRpp} builder software will work for a given collection is to try it. We suggest picking one or two granules/files and then following the steps outlined here in Section <<sec-build-them>> followed by the testing process described in Section <<sec-test-them>>. Are the variables all present? Are the values  (or a sampled subset of values) correct?

Support for HDF4 and HDF4-EOS2 data files is much newer, and more work will need to be done on edge cases than for HDF5. However, as of April 2025, the same advice applies to these as to the HDF5 case. Try to build the {DMRpp} and then test the result.

[TIP]
In NASA collections using HDF4-EOS2, geolocation information is often not included within individual data files. This approach minimizes storage requirements by avoiding the repeated storage of redundant information. For instance, a MODIS collection may contain approximately 10,000 files (granules), each referencing geolocation data drawn from a common set of around 120 predefined global regions. To manage this, {DMRpp} generates and stores the geo-referencing information in additional compressed data files, but without an attempt to limit that to the minimum amount of the geo-referencing data. Efforts to optimize the storage of HDF4-EOS2 geo-referencing data are planned and will be prioritized based on user demand.

==== Is My NetCDF File A Version _3_ or Version _4_ File?
OPeNDAP's {DMRpp} software does not currently support netCDF3 files.footnote:[Not supporting netCDF3 is a shame because it's commonly found in older collections of data and it's one of the simpler data formats.] A complicating factor in building {DMRpp} documents is that it can be hard to tell at a glance if a file is netCDF version 3 or version 4. A file with the suffix _.nc4_ is conventionally recognized as a _netCDF-4_ file. However, the file suffix _.nc_ is ambiguous, since it is often used for both _netCDF-3_ and _netCDF-4_ files.

You can use the `ncdump` command to determine if a _netCDF_ file is either classic _netCDF-3_ or _netCDF-4_ http://www.bic.mni.mcgill.ca/users/sean/Docs/netcdf/guide.txn_79.html[(You can learn more in the NetCDF documentation here)]. Here are two files, both using the suffix `.nc` where the first is netCDF3 and the second is netCDF4.

[source,shell,linenums]
----
% ncdump -k fnoc1.nc
classic

% ncdump -k SMAP_L4_SM_aup_20150420T210000_Vv7032_001.nc
netCDF-4
----

=== Summary of the {DMRpp} Build Process
[#sec-build-them]
==== Assumptions
You have:

* Docker installed on your computer and at least a basic understanding of its use.
* Data files in a directory on your computer

[NOTE]
In the following, `%` is the terminal prompt. Only some commands produce output, and for those that do, the output is shown below the command. The paths, etc., on your computer will almost certainly be different.

==== Examples
[#sec-examples]
In this section we jump right into some examples without much explanation. This shows the minimum amount of work needed to build the {DMRpp} and sidecar files. See <<sec-cmd-exp>> for details about the `gen_dmrpp_side_car` command, which is the recommended command for building {DMRpp} documents (April 2025).

Change to the directory that holds your data files and assign an environment variable to the full pathname of that directory. This will streamline some of the later steps in this section. In my case that directory is called `HDF4-dir`, and I used the environment variable 'DATA.'

[source,console,linenums]
[#ex-setup]
----
% cd HDF4-dir
% export DATA=$(pwd)
% echo $DATA
/Users/jimg/src/opendap/hyrax_git/HDF4-dir
----

Here are the files on my computer in the directory assigned to $DATA

[source,sh]
[#ex-dir-listing]
----
% ls
3B42.19980101.00.7.HDF
3B42.19980101.03.7.HDF
3B42.19980101.06.7.HDF
3B42.19980101.09.7.HDF
3B42.20130111.06.7.HDF
3B42.20130111.09.7.HDF
AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf
AIRS.2009.01.02.L3.RetStd_IR001.v7.0.3.0.G20160024358.hdf
AIRS.2009.01.03.L3.RetStd_IR001.v7.0.3.0.G20160024538.hdf
AMSR_E_L2_Land_V09_200206191023_D.hdf
AMSR_E_L2_Land_V09_200206191112_A.hdf
AMSR_E_L3_SeaIce25km_V15_20020601.hdf
MCD12Q1.A2022001.h10v06.061.2023243073808.hdf
MCD19A1.A2024025.h10v06.061.2024027100206.hdf
MOD10A1F.A2024025.h01v08.061.2024027134335.hdf
MOD10A1F.A2024025.h01v09.061.2024027130238.hdf
MOD10A1F.A2024025.h01v10.061.2024027131939.hdf
MOD11A1.A2024025.h10v06.061.2024028004317.hdf
----

Run the Docker container. The docker run command returns the Container ID (a long hexadecimal string) when the `-d` (run a detached container) is used. The `--name` option sets _hyrax_ as the name of the container which will be used in later commands. Running the container this way enables us to use both build {DMRpp} documents and later test them.

[source,console]
----
% docker run -d -h hyrax -p 8080:8080 -v $DATA:/usr/share/hyrax --name=hyrax opendap/hyrax:1.17.1-126
9c88a0d4abe55f17802afd81150280073314f3940b9cd4973ea60dbc43f733a9
----

[NOTE]
If you want to use the latest version of the `gen+dmrpp_side_car` command, replace the version number in _opendap/hyrax:1.17.1-126_ with _snapshot_. Using _opendap/hyrax:snapshot_ will always get the most recent version of the software.

To build a {DMRpp} for the first AIRS file we can run the `gen_dmrpp_side_car` command, using `docker exec`, with the file's name. Because this file is an HDF4 file, the command option `-H` is used.

.Building a {DMRpp} for an AIRS HDF4 file/granule.
[source,consolehighlight=7]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf -H

% ls
...
3B42.20130111.09.7.HDF
AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf
AIRS.2009.01.01.L3.RetStd_IR001.v7.0.3.0.G20160024306.hdf.dmrpp
AIRS.2009.01.02.L3.RetStd_IR001.v7.0.3.0.G20160024358.hdf
...
----

In this second example both the {DMRpp} and a sidecar _missing data_ file (`3B42.19980101.00.7.HDF_mvs.h5`) are built. As is often the case, the {DMRpp} and missing data file together are only 2% of the data file's size.

[NOTE]
Even though the input data file was an HDF4-ESO2 file, the missing data file uses HDF5 to store the values.

This is also an HDF4 file, so the `-H` option is used.

.Building both the {DMRpp} and a missing data file
[source,shell,linenums,highlight=6-7]
[#ex-missing]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i 3B42.19980101.00.7.HDF -H

% ls -l
total 1245840
-rw-r--r--@ 1 jimg  staff     774595 Aug 22  2024 3B42.19980101.00.7.HDF
-rw-r--r--  1 jimg  staff       6514 Apr 21 22:42 3B42.19980101.00.7.HDF.dmrpp
-rw-r--r--  1 jimg  staff       8075 Apr 21 22:42 3B42.19980101.00.7.HDF_mvs.h5
-rw-r--r--@ 1 jimg  staff     765742 Aug 22  2024 3B42.19980101.03.7.HDF
 ...
----

The final example in this section shows building a {DMRpp} for an HDF5 file. For an HDF5 file, do not include the `-H` option.

.Build a {DMRpp} for an HDF5 file.
[source,console,linenums,hightlight=10]
[#ex-hdf5]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5 -U

% ls -l
total 1895576
-rw-r--r--@ 1 jimg  staff     600255 Aug 22  2024 3B42.20190110.06.7.HDF
-rw-r--r--  1 jimg  staff       6595 Apr 22 17:19 3B42.20190110.06.7.HDF.dmrpp
-rw-r--r--  1 jimg  staff       8075 Apr 22 17:19 3B42.20190110.06.7.HDF_mvs.h5
 ...
-rw-r--r--@ 1 jimg  staff   95114159 Aug  5  2024 SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5
-rw-r--r--  1 jimg  staff     277290 Apr 25 15:51 SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5.dmrpp
----

==== How {DMRpp} References the Matching Data File
[#sec-data-source-url]
[TIP]
This section is primarily for NASA ESDIS users of the {DMRpp} document builder. However, there is some generally useful information here, so most readers should skim it over.

A {DMRpp} document is an eXtensible Markup Language (XML) document. We call the data file/granule that the {DMRpp} describes the _source data file_. Each {DMRpp} has at least one source data file, but may have more (for example, with HDF4-EOS2 data).  The first XML _element_ in the {DMRpp} contains a URL that points to the {DMRpp} document's source data file. It looks like this:

[source,xml,linenums,highlight=5]
----
<?xml version="1.0" encoding="ISO-8859-1"?>
<Dataset xmlns="http://xml.opendap.org/ns/DAP/4.0#"
    xmlns:dmrpp="http://xml.opendap.org/dap/dmrpp/1.0.0#" dapVersion="4.0" dmrVersion="1.0"
    name="SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
    dmrpp:href="https://test.opendap.org/examples/SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
    dmrpp:version="3.21.1-243">
----

There are three _XML attributes_ in the root element of the {DMRpp} that are relevant to this discussion. They are:

[source,shell]
----
name="SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
dmrpp:href="https://test.opendap.org/examples/SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
dmrpp:version="3.21.1-243">
----

[horizontal]
name:: The name of the data file/granule.
dmrpp:href:: The full URL to the source data file.
dmrpp:version:: The version of the {DMRpp} builder software used to make this {DMRpp} document.

The value of the `dmrpp:href` attribute is the source of data values that the Hyrax data server will use with building data responses. This URL can be either an HTTP, HTTPS or _file://_ URL (for more about the latter option, see Section <<sec-testing>>).

However, when the OPeNDAP {DMRpp} was first developed for use by NASA ESDIS, we did not want to encode the URl to the data file into the {DMRpp}. Instead, we planned on using the ESDIS Common Metadata Repository (CMR) to look up information about a granule and use that to find the source data file. This helped guard against having to edit many of the documents while the ESDIS system was in flux (i.e., it was a decision well aligned with agile development principles). In place of an explicit URL to the source data file, the `gen_dmrpp_side_car` will, by default, use a template string that the hyrax data server substitutes at runtime with the current data source URL as read from CMR.

What if you do not need or want that? The `-u` option of `gen_dmrpp_side_car` provides a way to tell the {DMRpp} document builder to use a specific value for the data source URL. The following examples show the {DMRpp} XML _with_ the template value for the data source URL and then using a URL set with the `-u` option.

.With the template
[source,shell]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5
%head  SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5.dmrpp
<?xml version="1.0" encoding="ISO-8859-1"?>
<Dataset xmlns="http://xml.opendap.org/ns/DAP/4.0#" xmlns:dmrpp="http://xml.opendap.org/dap/dmrpp/1.0.0#" dapVersion="4.0" dmrVersion="1.0"
    name="SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
    dmrpp:href="OPeNDAP_DMRpp_DATA_ACCESS_URL"
    dmrpp:version="3.21.1-243">
----

The template value for the data source URL is `OPeNDAP_DMRpp_DATA_ACCESS_URL`

.Explicit data source URL, set using `-u`
[source,shell]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5 -u https://test.opendap.org/examples/SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5
% head SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5.dmrpp
<?xml version="1.0" encoding="ISO-8859-1"?>
<Dataset xmlns="http://xml.opendap.org/ns/DAP/4.0#" xmlns:dmrpp="http://xml.opendap.org/dap/dmrpp/1.0.0#" dapVersion="4.0" dmrVersion="1.0"
    name="SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
    dmrpp:href="https://test.opendap.org/examples/SMAP_L4_SM_aup_20150420T210000_Vv7032_001.h5"
    dmrpp:version="3.21.1-243">
----

The `-u` option provides the literal text for the value of the `dmrpp:href` XML attribute.

==== How {DMRpp} References a Sidecar File for Geo-referencing Data
[#sec-sidecar-template]
The mechanism described above for the data source URL, where the {DMRpp} builder provides a template value for the data source URL _unless told otherwise_ using the `-u` option, is repeated for any necessary references to sidecar geo-referencing data. In this case the template value is `OPeNDAP_DMRpp_SC_DATA_ACCESS_URL` and the `-s` option (described below in Section <<sec-cmd-exp>>) should be used to override the default and provide a specific URL.

There is one exception to the rule that `-u` is used for the data source URL and `-s` is used for the sidecar data file. If `-u` is used, that name will be used as a _pattern_ for the sidecar data file such that the missing data file will be assumed to be named the same as the data source, but with the suffix `_mvs.h5`.

In this example, we show the three files made from an HDF4-EOS2 file that where the sidecar file is necessary. The output of the command is shown first, followed by two views inside the {DMRpp} document.

.An Explicit Data Source URL is a Pattern for an Explicit Sidecar Data URL
[source,shell]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i 3B42.20190110.06.7.HDF -H -u file:///usr/share/hyrax/3B42.20190110.06.7.HDF

% ls -l
total 1895672
-rw-r--r--@ 1 jimg  staff     600255 Aug 22  2024 3B42.20190110.06.7.HDF
-rw-r--r--  1 jimg  staff       6595 Apr 25 17:21 3B42.20190110.06.7.HDF.dmrpp
-rw-r--r--  1 jimg  staff       8075 Apr 25 17:21 3B42.20190110.06.7.HDF_mvs.h5
----

.The Resulting XML, edited. Look for the _file:///_ URLs marked with the comments _HERE_.
[source,xml]
----
<?xml version="1.0" encoding="ISO-8859-1"?>
<Dataset xmlns="http://xml.opendap.org/ns/DAP/4.0#" xmlns:dmrpp="http://xml.opendap.org/dap/dmrpp/1.0.0#"
    dapVersion="4.0" dmrVersion="1.0"
    name="3B42.20190110.06.7.HDF"
    dmrpp:href="file:///usr/share/hyrax/3B42.20190110.06.7.HDF">                    <!-- HERE -->
    <Dimension name="nlon" size="1440"/>
    <Dimension name="nlat" size="400"/>
    <Float32 name="nlat">
        ...
        <dmrpp:chunks compressionType="deflate" deflateLevel="4" fillValue="0" byteOrder="LE">
            <dmrpp:chunkDimensionSizes>400</dmrpp:chunkDimensionSizes>
            <dmrpp:chunk offset="5435" nBytes="636" chunkPositionInArray="[0]"
                href="file:///usr/share/hyrax/3B42.20190110.06.7.HDF_mvs.h5" />     <!-- HERE -->
        </dmrpp:chunks>
        ...
----

==== Explanation of the `gen_dmrpp_side_car` Command Options
[#sec-cmd-exp]
The gen_dmrpp_side_car command takes a few options that control how it builds {DMRpp} and sidecar files.
[horizontal,labelwidth=11]

-i:: The `-i` option is used to name the _input data file_. This data file should be found in the directory where the command is being run, or one of its child directories. In the latter case, the relative pathname to the file should be used. This option is required.

-H:: The `-H` option tells the command that the input file is an HDF4 or HDF4-EOS2 data file. If the `-H` option is not used, then the data file is assumed to be either HDF5 or netCDF4.

-c:: The `-c` option results in {DMRpp} and sidecar files that follow the Climate Forecast (CF) conventions. Using this option provides a {DMRpp} that mimics the behavior of the Hyrax server when it is used to serve data stored on POSIX file systems with the _EnableCF_ option turned on. This organizes the presentation of the variables to follow CF and flattens the internal hierarchy of the data files, hiding any _Groups_.

-D:: The `-D` option will disable the build of a sidecar file, even when one would normally be required. The default is to build sidecar data files when needed.

-U:: Use the template value (`OPeNDAP_DMRpp_SC_DATA_ACCESS_URL`) for the value of the sidecar data file URL. This is the default.

-u/--URL:: The `-u/--URL` and `-s/SURL` options control how URLs are represented in the {DMRpp} document. It is possible to build a {DMRpp} before the location of the data file in S3, for example, is known. In this case, the URL that references the data file will be represented by a 'template' value and substituted into the {DMRpp} _when the document is used_, nominally by the Hyrax service at runtime (although other software can also do this substitution - it is a simple text replacement). See Section <<sec-data-source-url>>. If this option is used, no run-time substitution of the data source URL will be performed.

-s/--SURL:: The `-s/--SURL` option provides the same feature for the URL that references the sidecar geo-referencing data file. The Hyrax service _assumes_ that the data file URL can be determined by removing the suffix `.dmrpp` from the {DMRpp} URL. Similarly, it assumes that the sidecar data file URL can be found by replacing the `.dmrpp` suffix with `_mvs.h5`. See <<ex-missing>>. Note that these options can be used to provide real values for data file and sidecar data URls. In that case, the given values will be used in the {DMRpp} instead of the template values. No run-time substitution of the URLs will be performed.

==== Explanation of the `docker run` Command Options
[#sec-docker-exp]
In the Section <<sec-examples>> we used one docker command to start a container and then a second docker command to run the {DMRpp} builder inside that container. Here is an explanation of those commands in more detail. First, the container is started on the host computer.

[source,sh]
----
% docker run -d -h hyrax -p 8080:8080 -v $DATA:/usr/share/hyrax --name=hyrax opendap/hyrax:1.17.1-126
9c88a0d4abe55f17802afd81150280073314f3940b9cd4973ea60dbc43f733a9
----

The `docker run -d ...` command will run the Hyrax container on your computer (called the _host_ computer) in _detached_ mode. The Hyrax container includes both the complete Hyrax service and the `gen_dmrpp_side_car` command. Later this server will be used to test the {DMRpp} documents that are built.

The volume mount, from `$DATA` to `/usr/share/hyrax` mounts the current directory of the host computer running the container to the directory _/usr/share/hyrax_ inside the container. That directory is the root of the Hyrax server's data tree. This means that the data files in the `$DATA` directory will be accessible by the server running in the container without any other configuration.

Complete option summary:
[horizontal]
-d, --detach:: Run container in the background and print container ID
-h, --hostname:: Set the container's host name
-p, --publish:: Publish a container's port(s) to the Docker host
-v, --volume:: Mount a volume so that the container can use files on the Docker host
--name:: Assign a name to the container; this name can be used in later Docker commands

Once running, the container is used to run the command that will build the {DMRpp} document.

[source,shell]
----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i 3B42.19980101.00.7.HDF -H -U
----

The command that built the {DMRpp} (and sidecar) file really consists of _two commands_. The first is `docker exec -it -w /usr/share/hyrax hyrax` which instructs docker to _execute_ a program in the running container named _hyrax_ and do so by first changing to the directory _/usr/share/hyrax_ in that container. By using the `-w` option we are able to run the gen_dmrpp_side_car command in the directory within the container where data appear.

The second command instructs the docker container to run `gen_dmrpp_side_car` using the arguments `-i 3B42.19980101.00.7.HDF -H -U` which mean use the file _3B42.19980101.00.7.HDF_ as the input data file, assume it is an HDF4 file and use the template name for the sidecar data file.

Complete option summary for the `docker exec` command:
[horizontal]
-i, --interactive:: Set the working directory inside the container
-t, --tty:: Allocate a pseudo-terminal
-w, --workdir:: Set the working directory inside the container

== Testing {DMRpp} Containers Using the Builtin Hyrax Server
[#sec-test-them]
One of the more confounding things about testing {DMRpp} documents is that it requires a data server, or some software component, that can interpret the documents. Instead of the data being directly available, the {DMRpp} sits between the software and the data. In this section we show how to test a {DMRpp} document that using the Hyrax server running in the container used to build the {DMRpp} document. To do this, we will build the {DMRpp} with _file URLs_ for the data and sidecar files instead of _HTTP URLs_ or the _template values_ that the command would normally use.

----
% docker exec -it -w /usr/share/hyrax hyrax gen_dmrpp_side_car -i 3B42.20130111.09.7.HDF -H -u 'file:///usr/share/hyrax/3B42.20130111.09.7.HDF'
----

Copy that pattern for whatever file you use. From the `/usr/share/hyrax` directory, you pass _get_dmrpp_h4_ the name of the file (because it's local to the current directory) using the `-i` option. The `-u` option tells the command to embed the URL that follows it in the {DMRpp}. I've used a _file://_  URL to the file _/usr/share/hyrax/3B42.19980101.00.7.HDF_.

NOTE: In the URL above, three slashes follow the colon: two from the way a URL names a protocol and one because the pathname starts at the root directory.

Let's look at how the _hyrax_ service will treat that data file using the {DMRpp}. In a browser, go to  http://localhost:8080/opendap/[http://localhost:8080/opendap/]. The _hyrax_ container must be started using the `docker run` command for this to work (Section <<sec-examples>>).

.Hyrax Catalog view of all files available.
image::Hyrax-including-new-DMRpp.png[width=650, height=400]

NOTE: The server caches data catalog information for 5 minutes (although this can be configured) so new items (e.g., {DMRpp} documents) may not show up right away. To force the display of a {DMRpp} that you just created, click on the source data file name and edit the URL so that the suffix `.dmr.html` is replaced by `.dmrpp.dmr` .

Click on your equivalent of the `3B42.20130111.09.7.HDF` link, subset, download, and open in Panoply or the equivalent.

.Page view of the DAP _Data Request Form_ for subsetting the dataset.
image::Hyrax-subsetting.png[width=650, height=400]

Below is a comparison of the same underlying data, the left window shows the data returned using the {DMRpp}, the right shows the data read directly from the file using the server's builtin HDF4 reader.

.Comparison of responses from a {DMRpp} and the native file handler.
image::Data-comparison.png[width=650, height=400]

== Serving Data Using {DMRpp} Files ==
[NOTE]
This is older text that repeats some of the above material, but it provides a good reference for using the {DMRpp} in a range of data provider situations.

There are three fundamental deployment scenarios for using {DMRpp} files to serve data with the Hyrax data server.

This can be simple categorized as follows:
The {DMRpp} file(s) are XML files that contain a root `dap4:Dataset` element with a `dmrpp:href` attribute whose value is one of:

. A http(s):// URL referencing to the underlying granule files via http.

. A file:// URL that references the granule file on the local filesystem in a location that is inside the BES' data root tree.

. The template string `OPeNDAP_DMRpp_DATA_ACCESS_URL`

Each will be discussed in turn below.

NOTE: By default, Hyrax will automatically associate files whose name ends with ".dmrpp" with the *{DMRpp}* handler.

=== Using {DMRpp} with HTTP/S URLs ===

If the {DMRpp} files that you wish to serve contain `dmrpp:href` attributes whose values are http(s) URLs then there are 2+1 steps to serve the data:

. Place the {DMRpp} files on the local disk inside the directory tree identified by the `BES.Catalog.catalog.RootDirectory` in the BES configuration.
. Ensure that the Hyrax `AllowedHosts` list is configured to allow Hyrax to access those target URLs. This can be accomplished by adding new regex records to the `AllowedHosts` list in `/etc/bes/site.conf`, creating that file as need be.
. If the data URLs require authentication to access, then you'll need to configure Hyrax for that too. See link:https://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html[The Hyrax Data Server Installation and Configuration Guide] for more information.


=== Using {DMRpp} with file URLs ===

Using {DMRpp} files with locally held files can be useful for verifying that {DMRpp} functionality is working without relying on network access that may have data rate limits, authenticated access configuration, or security access constraints. Additionally, in many cases the {DMRpp} access to the locally held data may be faster than through the native `netcdf-4/HDF5` data handlers.

In order to use {DMRpp} files that contain file:// URLs:
. Place the {DMRpp} files on the local disk inside the directory tree identified by the `BES.Catalog.catalog.RootDirectory` in the BES configuration.
. Ensure that the {DMRpp} files contain only file:// URLs that refer to data granule files that are inside the directory tree identified by the `BES.Catalog.catalog.RootDirectory` in the BES configuration.

Note: For Hyrax, a correctly formatted file URL must start with the protocol `file://` followed by the full qualified path to the data granule, for example: 

`/usr/share/hyrax/ghrsst/some_granule.h5`

so that the completed URL will have three slashes after the first colon:

`file:///usr/share/hyrax/ghrsst/some_granule.h5`

=== Using {DMRpp} with the template string (NASA). ===
[TIP]
This is most relevant to the operation of the NASA ESDIS Hyrax in the Cloud server deployment.

Another way to serve {DMRpp} files with Hyrax is to build the {DMRpp} files *without* valid URLs but with a template string that is replaced at runtime. If no target URL is supplied to _get_drmpp_ at the time that the {DMRpp} is generated the template string: `*OPeNDAP_DMRpp_DATA_ACCESS_URL*` will be added to the file in place of the URL. The at runtime it can be replaced with the correct value.

Currently, the only implementation of this is Hyrax's NGAP service that, when deployed in the NASA NGAP cloud, will accept _REST__ URLs that are defined as having a URL path component with two mandatory and one optional parameters:

----------------------------------------------------
 MANDATORY: "/collections/UMM-C:{concept-id}"
 MANDATORY: "/granules/UMM-G:{GranuleUR}"
----------------------------------------------------

.Example Hyrax in the Cloud REST URL
[source]
----
https://opendap.earthdata.nasa.gov/collections/C1443727145-LAADS/granules/MOD08_D3.A2020308.061.2020309092644.hdf.nc
----

[horizontal]
UMM-C:{concept-id}:: /collections/C1443727145-LAADS
UMM-G:{GranuleUR}:: /granules/MOD08_D3.A2020308.061.2020309092644.hdf.nc

When encountering this type of URL Hyrax will decompose it and use the content to formulate a query to the NASA CMR to retrieve the data access URL for the granule and for the {DMRpp} file. It then retrieves the {DMRpp} file and injects the data URL so that data access can proceed as described above.

[TIP]
More on the REST Path can be found https://wiki.earthdata.nasa.gov/display/DUTRAIN/Feature+analysis%3A+Restified+URL+for+OPENDAP+Data+Access[here] ([.underline]#NOTE: You need the right permissions to access the previous URL#).

////
== Recipe: Building and testing {DMRpp} files ==
There are two recipes shown here, the first using a Hyrax docker container and a second using the container that is part of the NASA EOSDIS Cumulus task.

*_Prerequisites_*:

- The Docker daemon running on a system that also supports a shell (the examples use bash in this section).

=== Recipe: Building {DMRpp} files using a Hyrax docker container ===

. Acquire representative granule files for the collection you wish to import. Put them on the system that is running the Docker daemon. For this recipe we will assume that these files have been placed in the directory:

	/tmp/dmrpp

. Get the most up-to-date Hyrax docker image:

	docker pull opendap/hyrax:snapshot

. Start the docker container, mounting your data directory on to the docker image at `/usr/share/hyrax`:

	docker run -d -h hyrax -p 8080:8080 --volume /tmp/dmrpp:/usr/share/hyrax --name=hyrax opendap/hyrax:snapshot

. Get a first view of your data using `get_dmrpp` with its default configuration.

.. If you want you can build a {DMRpp} for an example "input_file" using a docker exec command:

	docker exec -it hyrax get_dmrpp -b /usr/share/hyrax -o /usr/share/hyrax/input_file.dmrpp -u "file:///usr/share/hyrax/input_file" "input_file"

.. Or if you want more scripting flexibility you can log in to the docker container to do the same:

... Login to the docker container:

	docker exec -it hyrax /bin/bash

... Change working dir to data dir: 

	cd /usr/share/hyrax

... Set the data directory to the current one (`-b $(pwd)`) and set the data URL (`-u`) to the fully qualified path to the input file.

	get_dmrpp -b $(pwd) -o foo.dmrpp -u "file://"$(pwd)"/your_test_file" "your_test_file"

NOTE: Now that you have made a dmr++ file, use the running Hyrax server to view and test it by pointing your browser at: http://localhost:8080/opendap/

[start=5]
. You can also batch process all of your test granules, if you want to go that route. The following script assumes your source data files end with '.h5'.

NOTE: The resulting *{DMRpp}* files should contain the correct file:// URLs and be correctly located so that they may be tested with the Hyrax service running in the docker instance.

------------------------------------------------------------------------------------
#!/bin/bash
# This script will write each output file as a sidecar file into 
# the same directory as its associated input granule data file.

# The target directory to search for data files 
target_dir=/usr/share/hyrax
echo "target_dir: $target_dir";

# Search the target_dir for names matching the regex \*.h5 
for infile in `find "$target_dir" -name \*.h5`
do
    echo " Processing: $infile"

    infile_base=`basename "${infile}"`
    echo "infile_base: $infile_base"

    bes_dir=`dirname "${infile}"`
    echo "    bes_dir: $bes_dir"

    outfile="$infile.dmrpp"
    echo "     Output: $outfile"

    get_dmrpp -b "$bes_dir" -o "$outfile" -u "file://$infile" "$infile_base"
done
------------------------------------------------------------------------------------

TIP: Remember that you can use the Hyrax server that is running in the docker container to view and test the {DMRpp} files you just created by pointing your browser at: http://localhost:8080/opendap/


=== Testing and qualifying {DMRpp} files ===
In the previous section/step we created some initial {DMRpp} files using the default configuration. It is crucial to make sure that they provide the representation of the data that you and your users are expecting, and that they will work correctly with the Hyrax server. (See the following sections for details). If the generated {DMRpp} files do not match expectations then the default configuration of the `get_dmrpp` may need to be amended using the `-s` parameter.
If the data are currently being served by your DAAC's on-prem team this is where understanding exactly what the localizations made to the configurations of the on-prem Hyrax instances deployed for the collection is important. These localization will probably need to be injected into `get_drmpp` in order to produce the correct data representation in the {DMRpp} files.


=== Flattening Groups ===
By default `get_dmrpp` will preserve and show group hierarchies. If this is not desired, say for CF-1.0 compatibility, then you can change this by creating a small amendment to `get_dmrpp`'s default configuration. 

First create the amending configuration file:

	echo "H5.EnableCF=true" > site.conf

Then, change the invocation of `get_dmrpp` in the above example by adding the `-s` switch:

	get_dmrpp -s site.conf -b `pwd` -o "$dmrpp_file" -u "file://"`pwd`"/$file" "$file"

And re-run the {DMRpp} production as shown above.



=== DAP representations ===
We have test and assurance procedures for DAP4 and DAP2 protocols below. Both are important. For legacy datasets the DAP2 request API is widely used by an existing client base and should continue to be supported. Since DAP4 subsumes DAP2 (but with somewhat different API semantics) It should be checked for legacy datasets as well. For more modern datasets that content DAP4 types such as Int64 that are not part of the DAP2 specification or implementations we will need to rely on eliding the instances of unmapped types, or return an error when this is encountered.


------------------------------------------------------
# Test Constants:
GRANULE_FILE="some_name.h5"
# Granule URL
gf_url="http://localhost:8080/opendap/$GRANULE_FILE"
------------------------------------------------------



==== Inspect the {DMRpp} files ====

Do the {DMRpp} files have the expected `dmrpp:href` URL(s)?

	head -2 "$GRANULE_FILE.dmrpp"

==== Check DAP4 DMR Response ====
Inspect `$gf_url.dmrpp.dmr`

. Get the document, save as `foo.dmr`:

	curl -L -o foo.dmr "$gf_url.dmr"

. Is each variable's data type correct and as expected?
. Are the associated dimensions correct?


==== DAP4 Check binary data response ====

For a particular granule named GRANULE_FILE and a particular variable, named VARIABLE_NAME (Where VARIABLE_NAME is a https://opendap.github.io/dap4-specification/DAP4.html#_fully_qualified_names[full qualified DAP4 name]):

    curl -L -o dap4_subset_file "$gf_url.dap?dap4.ce=VARIABLE_NAME"
    curl -L -o dap4_subset_dmrpp "$gf_url.dmrpp.dap?dap4.ce=VARIABLE_NAME"
    cmp dap4_subset_file dap4_subset_dmrpp


==== DAP4 UI test ====

View and exercise the DAP4 Data Request Form `$gf_url.dmr.html` with a browser.

==== DAP2 Check DDS Response ====

. Inspect `$gf_url.dds`
.. Is each variable's data type correct and as expected?
.. Are the associated dimensions correct?
. Compare {DMRpp} DDS with granule file DDS -
For a particular granule named GRANULE_FILE and a particular variable named VARIABLE_NAME (Where VARIABLE_NAME is a https://zenodo.org/records/10794666[DAP2 name]):

    curl -L -o dap2_dds_file "$gf_url.dds"
    curl -L -o dap2_dds_dmrpp "$gf_url.dds"
    cmp dap2_dds_file dap2_dds_dmrpp


==== DAP2 Check binary data response ====

For a particular granule named GRANULE_FILE and a particular variable, VARIABLE_NAME (Where VARIABLE_NAME is a https://zenodo.org/records/10794666[DAP2 name]):


    curl -L -o dap2_subset_file "$gf_url.dods?VARIABLE_NAME"
    curl -L -o dap2_subset_dmrpp "$gf_url.dmrpp.dods?VARIABLE_NAME"
    cmp dap2_subset_file dap2_subset_dmrpp

NOTE: One might consider doing this with two or more variables. 

==== DAP2 UI Test ====

. View and exercise the DAP2 Data Request Form located here: `$gf_url.html`.
. Try it in Panoply! 
.. Open Panoply.
.. From the File menu select *Open Remote Dataset*...
.. Paste the `$gf_url.html` into the resulting dialog box.
////

[appendix]
== Useful Docker Commands
A useful docker command, `ps`, provides a way to see which docker containers are running.

[source,sh]
----
% docker ps
----
or make a command alais for a more compact listing than the default output of `docker ps`
[source,sh]
----
% alias d-ps='docker ps --format "table {{.ID}}\t{{.Names}}\t{{.Status}}\t{{.Image}}"'
----
This will show a somewhat easier-to-read bit of information about all the running Docker container on your host:
[source,sh]
----
% d-ps

CCONTAINER ID   NAMES     STATUS          IMAGE
82074fe6ccfe    hyrax     Up 13 minutes   opendap/hyrax:1.17.1-126
----
If you want to stop the container, use
[source,sh]
----
% docker rm -f hyrax
----

////
[appendix]
=== HDF5 ===
// TODO Rewrite this since, at this point all known NASA HDF5 files are supported. 4/22/25
The HDF5 data format is quite complex, and many of the options and edge cases are not currently supported by the {DMRpp} software.

These limitations and how to quickly evaluate a HDF5 or netCDF4 file for use with the {DMRpp} software are explained below.

==== HDF5 filters ====

The HDF5 format has several filter/compression options used for storing data values.
The {DMRpp} software currently supports data that use the  H5Z_FILTER_DEFLATE, H5Z_FILTER_SHUFFLE, and H5Z_FILTER_FLETCHER32 filters.
https://support.hdfgroup.org/documentation/HDF5/latest/group___h5_z.html[You can find more on HDF5 filters here.]

==== HDF5 storage layouts ====

The HDF5 format also uses a number of "storage layouts" that describe various structural organizations of the data values associated with a variable in the granule file.
The {DMRpp} software currently supports data that use the  H5D_COMPACT, H5D_CHUNKED, and H5D_CONTIGUOUS storage layouts. These are all the storage layouts defined by the HDF5 library, but others can be added.
https://support.hdfgroup.org/releases/HDF5/v1_16/v1_16_0/documentation/doxygen/_l_b_dset_layout.html[You can find more on HDF5 storage layouts here.]

==== Is my HDF5 or netCDF4 file suitable for {DMRpp}?
To get a human-readable assessment of the file that will show the storage layouts, chunking structure, and the filters needed for each variable (aka DATASET in the _HDF5_ vocabulary), use the https://support.hdfgroup.org/ftp/HDF5/documentation/doc1.6/Tools.html#Tools-Dump[h5dump] command line program.

.h5dump example output
[source,sh]
----
$ h5dump -H -p chunked_gzipped_fourD.h5
HDF5 "chunked_gzipped_fourD.h5" {
	GROUP "/" {
		DATASET "d_16_gzipped_chunks" {
			DATATYPE  H5T_IEEE_F32LE
			DATASPACE  SIMPLE { ( 40, 40, 40, 40 ) / ( 40, 40, 40, 40 ) }
			STORAGE_LAYOUT {
				CHUNKED ( 20, 20, 20, 20 )
				SIZE 2863311 (3.576:1 COMPRESSION)
			}
			FILTERS {
				COMPRESSION DEFLATE { LEVEL 6 }
			}
			FILLVALUE {
				FILL_TIME H5D_FILL_TIME_ALLOC
				VALUE  H5D_FILL_VALUE_DEFAULT
			}
			ALLOCATION_TIME {
				H5D_ALLOC_TIME_INCR
			}
		}
	}
}
----

=== HDF4 and HDF4-EOS2 ===
The internal data storage layout in an HDF4 file is more complex than that in an HDF5 file, and we're focusing on complete support for those features used by NASA. In addition, we also support HDF4-EOS2, data files that should be read with the HDF4-EOS2 library. The main reason of using the HDF-EOS2 API is to retrieve the values for the Domain variables such as  Latitude and Longitude. Our support handles the HDF4-EOS Grid data type and uses {DMRpp} to retrieve  the Latitude and Longitude values appear as users expect. Dmrpp can handle hDF-EOS2 swath. However, for some HDF-EOS2 MODIS swath (level 1B etc.), currently users need to find the corresponding HDF-EOS2 files (MODIS level 3 products) that store the actual latitude and longitude values for each data point. The {Dmrpp} module doesn't support  automatic merging of the latitude and longitude of such HDF-EOS2 swath data.

== Building {DMRpp} Documents
[NOTE]
The `gen_dmrpp_side_car` is a command line tool for building {DMRpp} documents introduced in March 2025 and is available only using the Hyrax Docker container version _1.17.1-126_ or later.

The `gen_dmrpp_side_car` command, introduced in March 2025, can be used to build {DMRpp} documents for HDF5, netCDF4, HDF4, and HDF-EOS2 data files. This command will also build _sidecar_ data files when needed that provide additional information that simplifies using the data in these files. For many of the NASA data collections, geo-referencing data were not included in the data files to reduce file size. The gen_dmrpp_side_car command will store the 'missing' geo-referencing data in a sidecar file and build a {DMRpp} document that automatically referencing that sidecar file, providing seamless access to those geo-referencing values.

[appendix]
== Using the New Builder Command
// From Kent in April 2025. jhrg 4/25/25

=== HDF4
To generate a dmrpp file for the HDF4 file hdf4.hdf. Do the following:
[source,sh]
----
gen_dmrpp_side_car -I hdf4.hdf -H -U
----
If a sidecar file is generated, the sidecar file is always named after the original HDF4 file plus `_mvs.h5`. For example, `hdf4.hdf_mvs.h5.`

NOTE: Note: `-H -U` are critical and cannot be omitted.

=== HDF5
To generate a dmrpp file for the HDF5 file `HDF5.h5`. Do the following:
[source,sh]
----
gen_dmrpp_side_car -i HDF5.h5  -U
----
////
